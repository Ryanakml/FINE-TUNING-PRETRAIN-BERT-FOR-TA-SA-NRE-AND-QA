{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "284516a5-86b3-4ebd-954c-c45520507fef",
   "metadata": {},
   "source": [
    "## BERT (Bidirectional Encoder Representation from Transformers)\n",
    "\n",
    "<img src=\"Screenshot 2025-05-09 at 01.27.33.png\" width=\"500\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbb336c7-edef-4540-b65d-99d73df21e71",
   "metadata": {},
   "source": [
    "- Apa itu Attention (udah kamu kuasai feeling-nya ðŸ”¥)\n",
    "\n",
    "- Bagaimana Attention dihitung (Query-Key-Value + hitungan kecil â†’ lagi kita lakuin sekarang)\n",
    "\n",
    "- Self-Attention di semua kata di kalimat (next step kita)\n",
    "\n",
    "- Multi-Head Attention (kenapa pakai banyak Attention barengan?)\n",
    "\n",
    "- Positional Encoding (gimana BERT tau urutan kata?) -> Learned Positional Embedding.\n",
    "\n",
    "- Encoder Stack (BERT = tumpukan encoder, kayak kue lapis)\n",
    "\n",
    "- Training Objective BERT (Masked Language Model + Next Sentence Prediction)\n",
    "\n",
    "- Fine-Tuning (gimana BERT dipakai buat tugas kayak QA, Sentimen, dst.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd253769-3ec8-46f3-81f5-815df672582f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# SCALED DOT PRODUCT ATTENTION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8777edb-9d53-4232-bc57-1852afaa220f",
   "metadata": {},
   "source": [
    "# FULL STACK ENCODER"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6a08790-0641-4999-8f92-d86c2f37709b",
   "metadata": {},
   "source": [
    "# PRETRAINING \n",
    "\n",
    "## MLM (MASKED LANGUAGE MODELLING)\n",
    "\n",
    "\n",
    "## NSP (NEXT SENTENCE PREDICTION)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ae2badb-d598-4c51-895f-1bede169ad59",
   "metadata": {},
   "source": [
    "# FINE-TUNING\n",
    "\n",
    "## Text Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5140e90-7f05-4e28-bf86-18d9defce5e6",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    BertTokenizer,\n",
    "    BertForSequenceClassification,\n",
    "    TrainingArguments,\n",
    "    Trainer\n",
    ")\n",
    "\n",
    "# Check if GPU is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Load pretrained model and tokenizer\n",
    "model_name = \"bert-base-uncased\"\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "model = BertForSequenceClassification.from_pretrained(model_name, num_labels=2)\n",
    "model.to(device)  # Move model to appropriate device\n",
    "\n",
    "# Load dataset\n",
    "dataset = load_dataset(\"imdb\", split=\"train[:2000]\")\n",
    "dataset = dataset.train_test_split(test_size=0.2)\n",
    "print(f\"Train dataset size: {len(dataset['train'])}\")\n",
    "print(f\"Test dataset size: {len(dataset['test'])}\")\n",
    "\n",
    "# Tokenize function - create a function that properly formats data\n",
    "def tokenize_function(examples):\n",
    "    # Return tokenized examples with proper padding and truncation\n",
    "    return tokenizer(\n",
    "        examples[\"text\"],\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=512,  # Specify max_length explicitly\n",
    "        return_tensors=\"pt\"  # Return PyTorch tensors\n",
    "    )\n",
    "\n",
    "# Process dataset to have the right format and data types\n",
    "tokenized_datasets = dataset.map(\n",
    "    tokenize_function,\n",
    "    batched=True,\n",
    "    remove_columns=[\"text\"]  # Remove text column which won't be needed\n",
    ")\n",
    "\n",
    "# Convert label column to make it compatible with the model\n",
    "tokenized_datasets = tokenized_datasets.map(\n",
    "    lambda examples: {\"labels\": examples[\"label\"]},\n",
    "    remove_columns=[\"label\"]  # Remove original label column\n",
    ")\n",
    "\n",
    "# Check the structure of our processed dataset\n",
    "print(\"\\nSample from processed dataset:\")\n",
    "sample = tokenized_datasets[\"train\"][0]\n",
    "print(f\"Type: {type(sample)}\")\n",
    "print(f\"Keys: {sample.keys()}\")\n",
    "print(f\"Example item: {sample}\")\n",
    "\n",
    "# Define training arguments\n",
    "# Check the TrainingArguments available parameters\n",
    "from inspect import signature\n",
    "print(\"Available parameters for TrainingArguments:\", signature(TrainingArguments))\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    # Try with no evaluation strategy parameter first\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    # Simplified arguments to minimize potential issues\n",
    "    logging_dir=\"./logs\"\n",
    ")\n",
    "\n",
    "# Define trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"test\"],\n",
    ")\n",
    "\n",
    "# Train model\n",
    "print(\"\\nStarting training...\")\n",
    "trainer.train()\n",
    "\n",
    "# Evaluate model\n",
    "print(\"\\nEvaluating model...\")\n",
    "eval_results = trainer.evaluate()\n",
    "print(f\"Evaluation results: {eval_results}\")\n",
    "\n",
    "# Save model\n",
    "model_path = \"./imdb-bert-classifier\"\n",
    "model.save_pretrained(model_path)\n",
    "tokenizer.save_pretrained(model_path)\n",
    "print(f\"\\nModel saved to {model_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05623054-3888-445f-ba9c-7a678188de15",
   "metadata": {},
   "source": [
    "## SQuAD "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d742dd0b-7e54-466a-a290-b522ca96adfb",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "from transformers import BertTokenizerFast, BertForQuestionAnswering\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from datasets import load_dataset\n",
    "\n",
    "\n",
    "model_name = \"bert-base-uncased\"\n",
    "\n",
    "tokenizer = BertTokenizerFast.from_pretrained(model_name)\n",
    "model = BertForQuestionAnswering.from_pretrained(model_name)\n",
    "\n",
    "\n",
    "dataset = load_dataset(\"squad\", split=\"train[:1000]\")\n",
    "dataset = dataset.train_test_split(test_size=0.2)\n",
    "\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    questions = [q.strip() for q in examples[\"question\"]]\n",
    "    contexts = [c.strip() for c in examples[\"context\"]]\n",
    "\n",
    "    inputs = tokenizer(\n",
    "        questions,\n",
    "        contexts,\n",
    "        max_length=384,\n",
    "        truncation=\"only_second\",\n",
    "        padding=\"max_length\",\n",
    "        return_offsets_mapping=True,\n",
    "        return_tensors=\"pt\",\n",
    "    )\n",
    "\n",
    "    start_positions = []\n",
    "    end_positions = []\n",
    "\n",
    "    for i, offsets in enumerate(inputs[\"offset_mapping\"]):\n",
    "        answer = examples[\"answers\"][i]\n",
    "        start_char = answer[\"answer_start\"][0]\n",
    "        end_char = start_char + len(answer[\"text\"][0])\n",
    "\n",
    "        sequence_ids = inputs.sequence_ids(i)\n",
    "\n",
    "        context_start = sequence_ids.index(1)\n",
    "        context_end = len(sequence_ids) - 1 - list(reversed(sequence_ids)).index(1)\n",
    "\n",
    "        offsets = offsets[context_start:context_end+1]\n",
    "        for idx, (start, end) in enumerate(offsets):\n",
    "            if start <= start_char and end >= start_char:\n",
    "                start_positions.append(idx + context_start)\n",
    "            if start <= end_char and end >= end_char:\n",
    "                end_positions.append(idx + context_start)\n",
    "\n",
    "    inputs[\"start_positions\"] = start_positions\n",
    "    inputs[\"end_positions\"] = end_positions\n",
    "    return inputs\n",
    "\n",
    "tokenized_dataset = dataset.map(preprocess_function, batched=True, remove_columns=dataset[\"train\"].column_names)\n",
    "\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./qa-results\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=3e-5,\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    weight_decay=0.01,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset[\"train\"],\n",
    "    eval_dataset=tokenized_dataset[\"test\"],\n",
    ")\n",
    "\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bffa11b9-df02-4a3b-820c-385cbbb3a7a9",
   "metadata": {},
   "source": [
    "# Output\n",
    "\n",
    "## HIDDEN STATE (HIDDEN OUTPUT)\n",
    "\n",
    "## POOLER OUTPUT ([CLS])\n",
    "\n",
    "| Output        | Bentuk                                       | Gunanya                                                                                  |\n",
    "| :------------ | :------------------------------------------- | :--------------------------------------------------------------------------------------- |\n",
    "| Hidden States | `(batch_size, sequence_length, hidden_size)` | Tiap token punya representasi kontekstual. Cocok buat tagging per token (NER, QA).       |\n",
    "| Pooler Output | `(batch_size, hidden_size)`                  | Representasi seluruh kalimat. Cocok buat klasifikasi kalimat (Pos/Neg, Entailment, dst). |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cec4c009-d866-482c-8659-4133982b10f2",
   "metadata": {},
   "source": [
    "# HYPERPARAMETER\n",
    "\n",
    "| Hyperparameter | BERT Umum      | Kenapa              |\n",
    "| -------------- | -------------- | ------------------- |\n",
    "| Learning Rate  | 2e-5 atau 3e-5 | Supaya update halus |\n",
    "| Batch Size     | 8 atau 16      | Supaya hemat memori |\n",
    "| Epoch          | 2-4            | Cukup buat adaptasi |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "930244c6-70a7-4b92-bf33-3f69d9ae271f",
   "metadata": {},
   "source": [
    "# QA MANUALLY\n",
    "\n",
    "- Masukin input âž” dapat logits (start_logits, end_logits).\n",
    "\n",
    "- Cari index paling tinggi dari start_logits âž” itu awal jawaban.\n",
    "\n",
    "- Cari index paling tinggi dari end_logits âž” itu akhir jawaban.\n",
    "\n",
    "- Potong token dari start sampai end âž” decode jadi teks jawaban.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8428e907-a52c-4eaf-ac9a-4515d9dd667e",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/huggingface_hub/file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Answer: \n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizerFast, BertForQuestionAnswering\n",
    "import torch\n",
    "\n",
    "# Load model\n",
    "model_name = \"bert-base-uncased\"\n",
    "tokenizer = BertTokenizerFast.from_pretrained(model_name)\n",
    "model = BertForQuestionAnswering.from_pretrained(model_name)\n",
    "\n",
    "# Input\n",
    "context = \"The capital city of France is Paris.\"\n",
    "question = \"What is the capital of France?\"\n",
    "\n",
    "# Tokenize\n",
    "inputs = tokenizer(question, context, return_tensors=\"pt\")\n",
    "\n",
    "# Forward pass\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "\n",
    "start_logits = outputs.start_logits\n",
    "end_logits = outputs.end_logits\n",
    "\n",
    "# Get the highest scoring start and end token positions\n",
    "start_index = torch.argmax(start_logits)\n",
    "end_index = torch.argmax(end_logits)\n",
    "\n",
    "# Decode the answer\n",
    "answer_ids = inputs[\"input_ids\"][0][start_index : end_index + 1]\n",
    "answer = tokenizer.decode(answer_ids)\n",
    "\n",
    "print(f\"Predicted Answer: {answer}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef91650d-3454-4215-8cc0-6d68bd1fae9e",
   "metadata": {},
   "source": [
    "# MLM (MASKED LANGUAGE MODELLING)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "de9a7ebe-53ff-4cc6-8a93-0b81450dd669",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Token: france\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizerFast, BertForMaskedLM\n",
    "import torch\n",
    "\n",
    "# Load model\n",
    "model_name = \"bert-base-uncased\"\n",
    "tokenizer = BertTokenizerFast.from_pretrained(model_name)\n",
    "model = BertForMaskedLM.from_pretrained(model_name)\n",
    "\n",
    "# Text with mask\n",
    "text = \"The capital of [MASK] is Paris.\"\n",
    "\n",
    "# Tokenize\n",
    "inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "\n",
    "# Forward pass\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "\n",
    "logits = outputs.logits\n",
    "\n",
    "# Index of [MASK]\n",
    "mask_token_index = (inputs.input_ids == tokenizer.mask_token_id)[0].nonzero(as_tuple=True)[0]\n",
    "\n",
    "# Predicted token\n",
    "predicted_token_id = logits[0, mask_token_index].argmax(axis=-1)\n",
    "predicted_token = tokenizer.decode(predicted_token_id)\n",
    "\n",
    "print(f\"Predicted Token: {predicted_token}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c6b48b4-e157-4148-9bf9-30901ced6587",
   "metadata": {},
   "source": [
    "# MODEL : MINI BERT MANUALLY\n",
    "\n",
    "## ARCHITECTURE\n",
    "\n",
    "<img src=\"attachment:4e443c08-d5ca-4537-988c-c9a6cb29be46.png\" width=\"500\" />\n",
    "\n",
    "\n",
    "## INPUT DATA\n",
    "\n",
    "## TOKENIZATION\n",
    "\n",
    "- Transforming every word into token (int ID) using vocab\n",
    "\n",
    "- WordPiece for BERT, for example :\n",
    "\n",
    "- Kalimat: \"cat eating fish\"\n",
    "- Tokenized â†’ [11, 25, 38]\n",
    "- Vocab â†’ {1 : 11, 2 : 25, 3 : 38}\n",
    "\n",
    "\n",
    "## POSITIONAL EMBEDDINGS\n",
    "\n",
    "- every token will turn into vector representation + its position.\n",
    "\n",
    "- step :\n",
    "\n",
    "- word embedding lookup table n dimentional for every word\n",
    "\n",
    "- positional encoding lookup table for every position word and add with embedding value.\n",
    "\n",
    "- example :\n",
    "\n",
    "- Word embed [128-d]\n",
    "\n",
    "- Position embed [128-d]\n",
    "\n",
    "- Add embed-position â†’ we get [128-d] for every token.\n",
    "\n",
    "\n",
    "\n",
    "## STACK ENCODER\n",
    "\n",
    "#### MULTIHEAD ATTENTION\n",
    "\n",
    "- Multi head meaning every word has n perpective on how that word context is.\n",
    "\n",
    "##### SCALED DOT PRODUCT ATTENTION / SELF ATTENTION\n",
    "\n",
    "- every token look into another token and update its representation based on proporsition telling relationship.\n",
    "\n",
    "\n",
    "- Step :\n",
    "\n",
    "- Input Vector â†’ Linear projection to Q, K, V.\n",
    "\n",
    "- attention score softmax(QKáµ€ / âˆšd).\n",
    "\n",
    "- mutiply score Ã— V.\n",
    "\n",
    "- Concacinate with another head\n",
    "\n",
    "- Output: new hidden state that more \"smart\" - has more context.\n",
    "\n",
    "\n",
    "#### RESIDUAL AND NORMALIZATION\n",
    "\n",
    "- jaga info lama + stabilkan training.\n",
    "- Tambahkan input + output Self-Attention âž” Residual.\n",
    "- Normalisasi hasilnya âž” LayerNorm.\n",
    "- attention_output = LayerNorm(input + attention(input))\n",
    "\n",
    "\n",
    "#### FEED FORWARD\n",
    "\n",
    "- Perbaiki representasi token secara individual.\n",
    "\n",
    "- proses :\n",
    "\n",
    "- expand â†’ relu â†’ shrink.\n",
    "\n",
    "- Seleksi fitur-fitur penting per token.\n",
    "\n",
    "- detail :\n",
    "\n",
    "- Dense layer 1: expand dimensi (misal 128 âž” 512).\n",
    "\n",
    "- ReLU : buang informasi ga guna, fokus ke internal connection\n",
    "\n",
    "- Dense layer 2: balik ke dimensi awal (512 âž” 128).\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#### RESIDUAL AND NORMALIZATION\n",
    "\n",
    "- jaga info lama + stabilkan training.\n",
    "- Tambahkan input + output Self-Attention âž” Residual.\n",
    "- Normalisasi hasilnya âž” LayerNorm.\n",
    "- attention_output = LayerNorm(input + attention(input))\n",
    "- ffn_output = LayerNorm(attention_output + FFN(attention_output))\n",
    "\n",
    "\n",
    "## ANOTHER ENCODER BLOCK\n",
    "\n",
    "- input = ffn_output\n",
    "\n",
    "- ....\n",
    "\n",
    "-  ......\n",
    "\n",
    "- 12 - 24X\n",
    "\n",
    "## OUTPUT \n",
    "\n",
    "\n",
    "### HIDDEN STATE\n",
    "\n",
    "- Hidden States\t(batch_size, sequence_length, hidden_size)\n",
    "- Tiap token punya representasi kontekstual.\n",
    "- buat tagging per token (NER, QA).\n",
    "\n",
    "\n",
    "### POOLED STATE\n",
    "\n",
    "- Pooler Output (batch_size, hidden_size)\n",
    "- Representasi seluruh kalimat.\n",
    "- buat klasifikasi kalimat (Pos/Neg, Entailment, dst)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cf751d0-21da-4895-8ff5-2095ac98f9a3",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## PSEUDOCODE INPLEMENTATION\n",
    "\n",
    "### INPUT\n",
    "    ## input text or corpus\n",
    "    \n",
    "    c = \"x y z ....\"\n",
    "\n",
    "### TOKEN\n",
    "\n",
    "    # text into number by id or index\n",
    "    \n",
    "    t = [x_id, y_id, z_id, ...... , n_id]\n",
    "\n",
    "### POSITIONAL EMBEDDING\n",
    "\n",
    "    ## encode each token to embed and position\n",
    "    \n",
    "    for each token in t:\n",
    "    \n",
    "    embedding vector = word embedding matrix [token_id] -> vectorize the input token for each word\n",
    "    positional vector = positional embeedin matrix [position] -> vector that tell what the order of each token by poition\n",
    "    input that vectorized: input_vector = embedding vector + positional vector\n",
    "\n",
    "    ## result:\n",
    "    \n",
    "    vectors  = [input_vector_1, input_vector_2, input_vector_3, ......., input_vector_n]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### STACK ENCODER - 2 LAYER\n",
    "\n",
    "    ## for each encoder layer :\n",
    "\n",
    "        ## if multihead self attention :\n",
    "    \n",
    "        for each token in t :\n",
    "            linear projection 1, 2, 3 ...., n =  Q, K, V  from input vectors\n",
    "        \n",
    "        for each pair of token (i,j) : \n",
    "            attention score 1, 2, 3 ...., n = dot product (Q[i], K[j]) / sqrt (d_model = input vectors [-1])\n",
    "\n",
    "        for each token i:\n",
    "            normalize the value by softmax accross attention score\n",
    "            attention output 1, 2, 3 ...., n= sum over j (softmax_score[i][j] * V[j])\n",
    "\n",
    "            \n",
    "        ## if self attention block\n",
    "\n",
    "        for each token in t :\n",
    "            linear projection =  Q, K, V  from input vectors\n",
    "        \n",
    "        for each pair of token (i,j) : \n",
    "            attention score = dot product (Q[i], K[j]) / sqrt (d_model = input vectors [-1])\n",
    "\n",
    "        for each token i:\n",
    "            normalize the value by softmax accross attention score\n",
    "            attention output = sum over j (softmax_score[i][j] * V[j])\n",
    "\n",
    "\n",
    "        ## residual connection \n",
    "    \n",
    "        attention output = layer_norm(input + attention output)\n",
    "\n",
    "\n",
    "        ## feed forward \n",
    "        \n",
    "        for each token in attention output :\n",
    "            - expand dimention 2x : h1 : linear(attention output)\n",
    "            - focus on important information in that token only : relu : relu(h1)\n",
    "            - compress again 2x -> original : h2 / output : linear(relu)\n",
    "\n",
    "\n",
    "        ## residual connection \n",
    "    \n",
    "        output = layer_norm(attention output + output)\n",
    "\n",
    "    \n",
    "        ## Output\n",
    "    \n",
    "        input vetor in the next encoder layer - repeat the process from linear projection - output.\n",
    "\n",
    "\n",
    "### OUTPUT - STATE\n",
    "\n",
    "    ## output hidden state\n",
    "    \n",
    "    Return:\n",
    "    - Output hidden vectors for each token\n",
    "    \n",
    "    ## output pooled state\n",
    "    \n",
    "    Return:\n",
    "    - [CLS] token output for classification tasks (if needed)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7f9217e-1c88-4dd1-86d8-9b698b6866f3",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## SIMPLE PYTHON IMPLEMENTATION\n",
    "\n",
    "### STRUCTURE\n",
    "\n",
    "\n",
    "| Puzzle | Modul           | Isi                                                                                          |\n",
    "| :----: | :-------------- | :------------------------------------------------------------------------------------------- |\n",
    "|    1   | Basic Functions | - Matrix multiplication (manual) <br> - Softmax Activation Function (manual) <br> - Layer normalization (manual) |\n",
    "|    2   | Embedding       | - Word Embedding Lookup table <br> - Positional Encoding Lookup table                                     |\n",
    "|    3   | Self Attention  | - Linear projection (Q, K, V) of input <br> - Attention score calculation <br> - Output V             |\n",
    "|    4   | Feed Forward    | - Linear 1 (expand), ReLU, Linear 2 (compress again)                                                |\n",
    "|    5   | Encoder Layer   | - Residual connection + Norm after Attention <br> - Residual connection + Norm after FFN |\n",
    "\n",
    "\n",
    "### BASIC FUNCTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5af0544b-d426-472e-a381-e6ebd2b5f8f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dot Product Function\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def matmul(x,y): \n",
    "    \n",
    "    return np.dot(x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6dd1831b-700e-4064-be68-c71e22d11444",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Softmax Activation Funtion\n",
    "\n",
    "def softmax(x, axis=-1):\n",
    "    x_max = np.max(x, axis=axis, keepdims=True)\n",
    "    e_x = np.exp(x - x_max)\n",
    "    \n",
    "    return e_x / np.sum(e_x, axis=axis, keepdims=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1ba11831-60a6-410d-a519-f548d75d8870",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalization Layer Function\n",
    "\n",
    "def norm(x):\n",
    "    \n",
    "    eps = 1e-6\n",
    "    avg = np.mean(x, axis=-1, keepdims=True)\n",
    "    std = np.std(x, axis=-1, keepdims=True)\n",
    "\n",
    "    return (x - avg)/(std + eps)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02634077-8217-4090-9b64-9b0825ba8028",
   "metadata": {},
   "source": [
    "### EMBEDDING LAYER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "22a949d3-3f79-4e70-a045-4bb750d8553f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word Embedding Lookup Table\n",
    "\n",
    "class wordEmbedding:\n",
    "    def __init__(self, vocab_size, d_model):\n",
    "        self.vocab_size = vocab_size\n",
    "        self.d_model = d_model\n",
    "        self.embedding_table = np.random.rand(vocab_size, d_model)*0.01\n",
    "\n",
    "    def forward(self, token_id):\n",
    "        return self.embedding_table[token_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "eed734ea-d3a7-43c1-908b-de2f235372aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Positional Encoding Lookup table\n",
    "\n",
    "class positionalEncoding:\n",
    "    def __init__(self, seq_len, d_model):\n",
    "        self.seq_len = seq_len\n",
    "        self.d_model = d_model\n",
    "\n",
    "        position = np.arange(seq_len)[:, np.newaxis]\n",
    "        div_term = np.exp(np.arange(0, d_model, 2) * (-np.log(10000.0) / d_model))\n",
    "\n",
    "        pe = np.zeros((seq_len, d_model))\n",
    "        pe[:, 0::2] = np.sin(position * div_term)\n",
    "        pe[:, 1::2] = np.cos(position * div_term)\n",
    "\n",
    "        self.embedding_table = pe\n",
    "\n",
    "    def forward(self, position_token_id):\n",
    "        return self.embedding_table[position_token_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b5ce1b7d-d0ec-4c55-bd30-1ee038816262",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 3, 10)\n"
     ]
    }
   ],
   "source": [
    "# Example..\n",
    "\n",
    "vocab_size = 99\n",
    "d_model = 10\n",
    "max_seq = 5\n",
    "\n",
    "word_embed = wordEmbedding(vocab_size, d_model)\n",
    "position_embed = positionalEncoding(max_seq, d_model)\n",
    "\n",
    "corpus = [['i', 'like', 'pussies'], ['he', 'hate', 'dogs']]\n",
    "\n",
    "tokens = np.array([\n",
    "    [5, 8, 20],   # Sentence 1\n",
    "    [6, 9, 25]    # Sentence 2\n",
    "])  # shape: (2, 3)\n",
    "\n",
    "positions = np.array([0, 1, 2])  # for all sentences, same positions\n",
    "\n",
    "\n",
    "word_vecs = word_embed.forward(tokens)\n",
    "pos_vecs = position_embed.forward(positions)\n",
    "\n",
    "final_input = word_vecs + pos_vecs\n",
    "print(final_input.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bf1ff8d-a0a2-4fe0-aaf8-ff2c368ae1a2",
   "metadata": {},
   "source": [
    "### SCALED DOT PRODUCT ATTENTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ce867f88-e010-43a8-9a37-0cc605159019",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear projection (Q, K, V)\n",
    "# Attention score calculation\n",
    "# Output V\n",
    "\n",
    "class selfAttention:\n",
    "    def __init__(self, d_model):\n",
    "        self.d_model = d_model\n",
    "\n",
    "        scale = np.sqrt(2.0 / d_model)\n",
    "        self.w_q = np.random.rand(d_model, d_model) * scale\n",
    "        self.w_k = np.random.rand(d_model, d_model) * scale\n",
    "        self.w_v = np.random.rand(d_model, d_model) * scale\n",
    "\n",
    "    def forward(self, x):\n",
    "        q = matmul(x, self.w_q)\n",
    "        k = matmul(x, self.w_k)\n",
    "        v = matmul(x, self.w_v)\n",
    "\n",
    "        k_t = np.transpose(k, (0, 2, 1))\n",
    "\n",
    "        attn_scores = matmul(q, k_t)/np.sqrt(self.d_model)\n",
    "        attn_probs = softmax(attn_scores, axis=-1)\n",
    "        attn_output = matmul(attn_probs, v)\n",
    "\n",
    "        return attn_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b35eb48f-a285-4e9a-b41b-8a5a165e11a6",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 3, 1, 1, 10)\n"
     ]
    }
   ],
   "source": [
    "# Example : 3 kata, 64 dimensi\n",
    "\n",
    "x = np.random.randn(1, 3, 10)\n",
    "\n",
    "self_attn = selfAttention(d_model=10)\n",
    "\n",
    "out = self_attn.forward(x)\n",
    "\n",
    "print(out.shape)  # (1, 3, 64)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "941465c8-631a-49b1-b1cc-31ed7383bbc7",
   "metadata": {},
   "source": [
    "### MULTIHEAD ATTENTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4f5c3b85-8eca-4bb1-826f-b44398c03d7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multihead Attention\n",
    "\n",
    "class multiheadAttention():\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.heads_d = d_model // num_heads\n",
    "\n",
    "        assert d_model % num_heads == 0\n",
    "\n",
    "        # weigth Q, K, V\n",
    "        scale = np.sqrt(2.0 / d_model)\n",
    "        self.w_q = np.random.rand(d_model, d_model) * scale\n",
    "        self.w_k = np.random.rand(d_model, d_model) * scale\n",
    "        self.w_v = np.random.rand(d_model, d_model) * scale\n",
    "        self.w_o = np.random.rand(d_model, d_model) * scale\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, seq_len, _ = x.shape\n",
    "\n",
    "        # Q, K, V\n",
    "        q = np.matmul(x, self.w_q)\n",
    "        k = np.matmul(x, self.w_k)\n",
    "        v = np.matmul(x, self.w_v)\n",
    "\n",
    "        # split into multiple head\n",
    "        q = q.reshape(batch_size, seq_len, self.num_heads, self.heads_d)\n",
    "        k = k.reshape(batch_size, seq_len, self.num_heads, self.heads_d)\n",
    "        v = v.reshape(batch_size, seq_len, self.num_heads, self.heads_d)\n",
    "\n",
    "        # transpose\n",
    "        q = q.transpose(0,2,1,3)\n",
    "        k = k.transpose(0,2,1,3)\n",
    "        v = v.transpose(0,2,1,3)\n",
    "\n",
    "        k_t = np.transpose(k, (0, 1, 3, 2))\n",
    "\n",
    "        # self attention per head\n",
    "        attn_scores = np.matmul(q, k_t) / np.sqrt(self.heads_d)\n",
    "        attn_probs = softmax(attn_scores, axis=-1)\n",
    "        attn_output = np.matmul(attn_probs, v)\n",
    "\n",
    "        # concatinate all heads into one\n",
    "        attn_output = attn_output.transpose(0,2,1,3)\n",
    "        attn_output = attn_output.reshape(batch_size, seq_len, self.d_model)\n",
    "\n",
    "        # linear projection \n",
    "        output = np.matmul(attn_output, self.w_o)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "0dfd0ec1-2639-4560-9de4-71f4ee7d7c74",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input : [[[-0.77281009 -0.26525163  0.49480591 -0.71234687  1.55256265\n",
      "    0.03180343  0.97385523 -1.85170586]\n",
      "  [-0.57396265 -0.72822389 -1.92776488  1.28525226  0.62958119\n",
      "    0.74690941  0.74743178  0.4630312 ]\n",
      "  [ 0.27997036  0.23979904 -0.09229853 -0.70767435 -0.09800739\n",
      "   -0.34597603  1.06333414  0.25265971]\n",
      "  [-1.82387414  0.24315024 -0.23913714  0.07299795 -1.43867199\n",
      "    0.2218916   0.79692588 -2.53512674]]\n",
      "\n",
      " [[-0.99488304  0.85132085  0.08817293  1.058072    0.68034666\n",
      "   -0.88408008 -0.6120395  -0.78602775]\n",
      "  [ 0.68272199  0.24199436  0.38193769 -0.50565454 -0.26785863\n",
      "    1.48656754 -1.38883187 -0.58510711]\n",
      "  [-0.64977469  0.68919681  1.23141812  0.39152976 -1.00440959\n",
      "   -0.97901076 -1.48389504 -0.96477045]\n",
      "  [ 0.32549657  1.18308707 -0.0607689   1.34716348  0.15218075\n",
      "    0.48560845  0.78811011 -0.05491841]]]\n",
      "\n",
      "shape of input : (2, 4, 8)\n",
      "\n",
      "batch_size = sentences : 2\n",
      "seq_len = words : 4\n",
      "d_model = dimention : 8\n",
      "\n",
      "output : [[[-0.00044888 -0.00099372 -0.00042065 -0.00070045 -0.00093345\n",
      "   -0.00113741 -0.00064498 -0.00043073]\n",
      "  [-0.00044818 -0.0009925  -0.00042007 -0.00069967 -0.0009322\n",
      "   -0.00113622 -0.0006435  -0.00042987]\n",
      "  [-0.00044854 -0.00099315 -0.00042037 -0.00070006 -0.00093289\n",
      "   -0.00113679 -0.00064451 -0.00043034]\n",
      "  [-0.00045097 -0.00099701 -0.00042244 -0.00070298 -0.00093677\n",
      "   -0.00114098 -0.00064773 -0.00043307]]\n",
      "\n",
      " [[-0.00061781 -0.00045114 -0.00040297 -0.00049577 -0.00059227\n",
      "   -0.00043976 -0.00043851 -0.0005988 ]\n",
      "  [-0.00061808 -0.00045135 -0.00040318 -0.00049617 -0.00059253\n",
      "   -0.00043992 -0.00043878 -0.00059887]\n",
      "  [-0.0006196  -0.00045375 -0.00040454 -0.00049788 -0.00059495\n",
      "   -0.00044245 -0.00044084 -0.000601  ]\n",
      "  [-0.00061471 -0.00044659 -0.00040034 -0.0004924  -0.00058749\n",
      "   -0.00043502 -0.00043467 -0.0005948 ]]]\n",
      "\n",
      "expected_shape : (2, 4, 8)\n",
      "\n",
      "actual_shape : (2, 4, 8)\n"
     ]
    }
   ],
   "source": [
    "# Xample : sentences : 2, words : 4, d_model : 8, num of heads = 4 (4 konteks each word)\n",
    "\n",
    "batch_size = 2\n",
    "seq_len = 4\n",
    "d_model = 8\n",
    "num_heads = 4\n",
    "\n",
    "x = np.random.randn(batch_size, seq_len, d_model)  # (2, 4, 8)\n",
    "\n",
    "mha = multiheadAttention(d_model, num_heads)\n",
    "\n",
    "output = mha.forward(x)\n",
    "\n",
    "print(f\"input : {x}\")\n",
    "print()\n",
    "print(f\"shape of input : {x.shape}\")\n",
    "print()\n",
    "\n",
    "batch_size, seq_len, d_model = x.shape\n",
    "\n",
    "print(f\"batch_size = sentences : {batch_size}\")\n",
    "print(f\"seq_len = words : {seq_len}\")\n",
    "print(f\"d_model = dimention : {d_model}\")\n",
    "\n",
    "\n",
    "expected_shape = (batch_size, seq_len, d_model)\n",
    "actual_shape = output.shape\n",
    "\n",
    "print()\n",
    "print(f\"output : {output}\")\n",
    "print()\n",
    "print(f\"expected_shape : {expected_shape}\")\n",
    "print()\n",
    "\n",
    "print(f\"actual_shape : {actual_shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9799ea6c-b17b-44a3-aed1-d75b3a2e323e",
   "metadata": {},
   "source": [
    "### FFN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2f12ab84-96bc-4686-9a3f-e96e87ae527e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feed Forward Neural Network\n",
    "\n",
    "class feedForward:\n",
    "    def __init__(self, d_model, d_ff):\n",
    "        scale1 = np.sqrt(2.0 / d_model)\n",
    "        scale2 = np.sqrt(2.0 / d_ff)\n",
    "        \n",
    "        self.w1 = np.random.rand(d_model, d_ff) * scale1\n",
    "        self.b1 = np.zeros((d_ff,))\n",
    "        self.w2 = np.random.rand(d_ff, d_model) * scale2\n",
    "        self.b2 = np.zeros((d_model,))\n",
    "\n",
    "\n",
    "    def gelu(self, x):\n",
    "        return 0.5 * x * (1 + np.tanh(np.sqrt(2 / np.pi) * (x + 0.044715 * np.power(x, 3))))\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        x_expanded = np.matmul(x, self.w1) + self.b1\n",
    "        x_relu = self.gelu(x_expanded)\n",
    "        output = np.matmul(x_relu, self.w2) + self.b2\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "4c6a4e14-630d-45d3-8d4c-f2fb674949b5",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Input shape: (2, 3, 4)\n",
      "Output shape: (2, 3, 4)\n",
      "Output:\n",
      " [[[24.93228925 25.01672739 28.52430536 27.89781689]\n",
      "  [25.15489125 24.22581311 27.73751544 26.67556067]\n",
      "  [36.99291892 36.90875791 41.84860775 40.34837315]]\n",
      "\n",
      " [[26.06460181 25.73991014 29.46731452 28.66470778]\n",
      "  [35.34297198 34.76383522 39.95592629 39.16061508]\n",
      "  [23.08314383 22.70280699 25.94482227 25.20208626]]]\n"
     ]
    }
   ],
   "source": [
    "# === Test Code ===\n",
    "if __name__ == \"__main__\":\n",
    "    d_model = 4\n",
    "    d_ff = 64\n",
    "    batch_size = 2\n",
    "    seq_len = 3\n",
    "\n",
    "    # Buat dummy input (batch_size, seq_len, d_model)\n",
    "    x = np.random.rand(batch_size, seq_len, d_model)\n",
    "\n",
    "    # Inisialisasi Feed Forward\n",
    "    ff = feedForward(d_model, d_ff)\n",
    "\n",
    "    # Proses forward tiap token\n",
    "    out = np.array([[ff.forward(token) for token in sample] for sample in x])\n",
    "    \n",
    "    print()\n",
    "    print(\"Input shape:\", x.shape)\n",
    "    print(\"Output shape:\", out.shape)\n",
    "    print(\"Output:\\n\", out)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8901445b-5339-4fa6-9ce0-101720689695",
   "metadata": {},
   "source": [
    "### STACK ENCODER - 3 LAYER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "eac2a888-ee46-4655-a408-eb75ccf8d80e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class bertBlock:\n",
    "    def __init__(self, d_model, num_heads, d_ff):\n",
    "        self.mha = multiheadAttention(d_model, num_heads)\n",
    "        self.hh = feedForward(d_model, d_ff)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # multihead self attention\n",
    "        x_mha = self.mha.forward(x)\n",
    "\n",
    "        # add and norm\n",
    "        x_norm1 = norm(x + x_mha)\n",
    "\n",
    "        # feed forward\n",
    "        x_ffn = self.hh.forward(x_norm1) \n",
    "\n",
    "        # add and norm\n",
    "        x_norm2 = norm(x_norm1 + x_ffn)\n",
    "\n",
    "        return x_norm2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "bc4fa2a9-df12-4311-a5e0-214c9077bb3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class bertModel:\n",
    "    def __init__(self, vocab_size, seq_len, d_model, num_heads, d_ff, n_stacks):\n",
    "        self.word_embedding = wordEmbedding(vocab_size, d_model)\n",
    "        self.positional_embedding = positionalEncoding(seq_len, d_model)\n",
    "        self.bert_blocks = [bertBlock(d_model, num_heads, d_ff) for _ in range(n_stacks)]\n",
    "\n",
    "    def forward(self, token_id, position_token_id=None):\n",
    "        batch_size, seq_len = token_id.shape\n",
    "\n",
    "        if position_token_id is None:\n",
    "            position_token_id = np.tile(np.arange(seq_len), (batch_size, 1))\n",
    "\n",
    "        word_embedding = self.word_embedding.forward(token_id)\n",
    "        positional_embedding = self.positional_embedding.forward(position_token_id)\n",
    "\n",
    "        x = word_embedding + positional_embedding\n",
    "\n",
    "        for block in self.bert_blocks:\n",
    "            x = block.forward(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "032f81e0-5fdf-4b13-bf9c-b14f6cc1b945",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output shape: (2, 6, 768)\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Hyperparameters\n",
    "    vocab_size = 3000\n",
    "    seq_len = 512\n",
    "    d_model = 768\n",
    "    num_heads = 12\n",
    "    d_ff = 3072\n",
    "    n_stacks = 12\n",
    "    \n",
    "    # Create model instance\n",
    "    model = bertModel(vocab_size, seq_len, d_model, num_heads, d_ff, n_stacks)\n",
    "    \n",
    "    # Example input (batch_size=2, seq_len=6)\n",
    "    token_id = np.array([[1, 2, 3, 4, 5, 6], [7, 8, 9, 10, 11, 12]])\n",
    "    \n",
    "    # Forward pass\n",
    "    output = model.forward(token_id)\n",
    "    print(f\"Output shape: {output.shape}\")  # Should be (2, 6, 768)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee2b9425-a0ec-48f8-af4b-132bbd615405",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "| Step | Penjelasan                           | Status |\n",
    "| :--: | :----------------------------------- | :----: |\n",
    "|   1  | Bangun Mini-BERT Stack               |    âœ…   |\n",
    "|   2  | Pretraining (Masked LM + NSP)        |   ðŸ”œ   |\n",
    "|   3  | Fine-tuning ke task spesifik         |   ðŸ”œ   |\n",
    "|   4  | Buat dataset dummy buat latihan      |   ðŸ”œ   |\n",
    "|   5  | Build mindset & intuition level dewa |   ðŸ”œ   |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d7f697b-bacb-4add-a8e1-7505e48eda46",
   "metadata": {},
   "source": [
    "# Pretraining (Masked LM + NSP)\n",
    "\n",
    "INTI :\n",
    "\n",
    "- Input: Token yang di-mask sebagian + sepasang kalimat\n",
    "- Target 1: Isi kata yang di-mask\n",
    "- Target 2: Apakah kalimat kedua nyambung?\n",
    "\n",
    "HOW? :\n",
    "\n",
    "- Tokenisasi kalimat âž” jadi token ID\n",
    "\n",
    "- Tambahin [CLS] di awal, [SEP] antar kalimat\n",
    "\n",
    "- Tambahin Positional Encoding kayak biasa\n",
    "\n",
    "- Random pilih token buat di-[MASK] (sekitar 15% token)\n",
    "\n",
    "- Masukin ke Mini-BERT stack - model kita\n",
    "\n",
    "- Output 1: Prediksi isi token yang ketutup\n",
    "\n",
    "- Output 2: Prediksi label NSP (IsNext / NotNext)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "| Misi                              | Tujuan                         | Gampangnya                                     |\n",
    "| :-------------------------------- | :----------------------------- | :--------------------------------------------- |\n",
    "| 1. Masked Language Model (MLM)    | Belajar isi kata yang hilang   | Tebak kata yang ketutupan                      |\n",
    "| 2. Next Sentence Prediction (NSP) | Belajar hubungan antar kalimat | Tebak apakah kalimat kedua nyambung atau ngaco |\n",
    "\n",
    "\n",
    "# MLM : MASK LANGUAGE MODEL\n",
    "\n",
    "\n",
    "## INTUITION\n",
    "\n",
    "- Belajar isi kata yang hilang, tutup beberapa kata dalam kalimat\n",
    "- Tebak kata yang ketutupan , suruh bert nebak itu\n",
    "- Kalimat asli:\n",
    "- \"Saya makan nasi di warung.\"\n",
    "\n",
    "- Setelah masking:\n",
    "- \"Saya [MASK] nasi di [MASK].\"\n",
    "\n",
    "- Tugas BERT:\n",
    "- Tebak [MASK] = \"makan\", [MASK] = \"warung\"\n",
    "\n",
    "\n",
    "## PROCESS\n",
    "\n",
    "1. Input :\n",
    "\n",
    "- c = ['kucing bermain di taman']\n",
    "\n",
    "- t = ['kucing', 'bermain', 'di', 'taman']\n",
    "\n",
    "\n",
    "2. Special Token :\n",
    "\n",
    "- ['[CLS]', 'kucing', 'bermain', 'di', 'taman', '[SEP]']\n",
    "\n",
    "\n",
    "3. Masking 15% Input :\n",
    "\n",
    "- ['[CLS]', 'kucing', '[MASK]', 'di', 'taman', '[SEP]']\n",
    "\n",
    "4. Pretrain Model with this Approach :\n",
    "\n",
    "- Input : ['[CLS]', 'kucing', '[MASK]', 'di', 'taman', '[SEP]']\n",
    "  \n",
    "- Embedding (token embedding + positional embedding),\n",
    "  \n",
    "- Stack Encoder stack (MHA âž” AddNorm âž” FFN âž” AddNorm),\n",
    "\n",
    "- keluar tensor representasi semua token.\n",
    "\n",
    "\n",
    "## PSEUDOCODE\n",
    "\n",
    "    # pretraining bert for mlm\n",
    "    initialize bert model with random weight\n",
    "\n",
    "    def apply mask (tokens):\n",
    "        for i in range (len token):\n",
    "            if random < 0.15:\n",
    "                if random < 0.8:\n",
    "                    tokens[i] = [mask]\n",
    "                elif random < 0.9:\n",
    "                    token[i] = random_token()\n",
    "                else:\n",
    "                    token[i] = token[i]\n",
    "                lebel[i] = original token\n",
    "            else:\n",
    "                label[i] = [ignore]\n",
    "\n",
    "        return tokens, label\n",
    "    \n",
    "    for each epoch:\n",
    "        for each batch in training data :\n",
    "        # 1. tokenize\n",
    "        input token = tokenize(batch)\n",
    "\n",
    "        # 2. masking\n",
    "        mask input, label = apply mask (input token)\n",
    "\n",
    "        # 3. feed forward bert\n",
    "        output = bertmodel(mask input)\n",
    "\n",
    "        # 4. training, loss \n",
    "        loss = cross entropy(output[mask position], labels[mask position])\n",
    "\n",
    "        # 5. backpropagation or update parameter\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero grad()\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05f074b9-7ff0-4d44-b23b-e97f45b49293",
   "metadata": {},
   "source": [
    "## NEXT SENTENCE PREDICTION (NSP)\n",
    "\n",
    "- Dikasih dua kalimat, suruh BERT tebak:\n",
    "\n",
    "- Nyambung? (A âž” B)\n",
    "\n",
    "- Atau ngaco? (A âž” random)\n",
    "\n",
    "- Kalimat 1: \"Saya pergi ke pasar.\"\n",
    "- Kalimat 2: \"Saya membeli buah.\"\n",
    "- ==> Label: IsNext (nyambung)\n",
    "\n",
    "- Kalimat 1: \"Saya pergi ke pasar.\"\n",
    "- Kalimat 2: \"Bulan purnama sangat indah.\"\n",
    "- ==> Label: NotNext (acak)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe989b18-41f2-48ee-8f48-394a1e17081d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
