{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "284516a5-86b3-4ebd-954c-c45520507fef",
   "metadata": {},
   "source": [
    "## BERT (Bidirectional Encoder Representation from Transformers)\n",
    "\n",
    "<img src=\"Screenshot 2025-05-09 at 01.27.33.png\" width=\"500\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbb336c7-edef-4540-b65d-99d73df21e71",
   "metadata": {},
   "source": [
    "- Apa itu Attention (udah kamu kuasai feeling-nya ðŸ”¥)\n",
    "\n",
    "- Bagaimana Attention dihitung (Query-Key-Value + hitungan kecil â†’ lagi kita lakuin sekarang)\n",
    "\n",
    "- Self-Attention di semua kata di kalimat (next step kita)\n",
    "\n",
    "- Multi-Head Attention (kenapa pakai banyak Attention barengan?)\n",
    "\n",
    "- Positional Encoding (gimana BERT tau urutan kata?) -> Learned Positional Embedding.\n",
    "\n",
    "- Encoder Stack (BERT = tumpukan encoder, kayak kue lapis)\n",
    "\n",
    "- Training Objective BERT (Masked Language Model + Next Sentence Prediction)\n",
    "\n",
    "- Fine-Tuning (gimana BERT dipakai buat tugas kayak QA, Sentimen, dst.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd253769-3ec8-46f3-81f5-815df672582f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# SCALED DOT PRODUCT ATTENTION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8777edb-9d53-4232-bc57-1852afaa220f",
   "metadata": {},
   "source": [
    "# FULL STACK ENCODER"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6a08790-0641-4999-8f92-d86c2f37709b",
   "metadata": {},
   "source": [
    "# PRETRAINING \n",
    "\n",
    "## MLM (MASKED LANGUAGE MODELLING)\n",
    "\n",
    "\n",
    "## NSP (NEXT SENTENCE PREDICTION)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ae2badb-d598-4c51-895f-1bede169ad59",
   "metadata": {},
   "source": [
    "# FINE-TUNING\n",
    "\n",
    "## Text Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5140e90-7f05-4e28-bf86-18d9defce5e6",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    BertTokenizer,\n",
    "    BertForSequenceClassification,\n",
    "    TrainingArguments,\n",
    "    Trainer\n",
    ")\n",
    "\n",
    "# Check if GPU is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Load pretrained model and tokenizer\n",
    "model_name = \"bert-base-uncased\"\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "model = BertForSequenceClassification.from_pretrained(model_name, num_labels=2)\n",
    "model.to(device)  # Move model to appropriate device\n",
    "\n",
    "# Load dataset\n",
    "dataset = load_dataset(\"imdb\", split=\"train[:2000]\")\n",
    "dataset = dataset.train_test_split(test_size=0.2)\n",
    "print(f\"Train dataset size: {len(dataset['train'])}\")\n",
    "print(f\"Test dataset size: {len(dataset['test'])}\")\n",
    "\n",
    "# Tokenize function - create a function that properly formats data\n",
    "def tokenize_function(examples):\n",
    "    # Return tokenized examples with proper padding and truncation\n",
    "    return tokenizer(\n",
    "        examples[\"text\"],\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=512,  # Specify max_length explicitly\n",
    "        return_tensors=\"pt\"  # Return PyTorch tensors\n",
    "    )\n",
    "\n",
    "# Process dataset to have the right format and data types\n",
    "tokenized_datasets = dataset.map(\n",
    "    tokenize_function,\n",
    "    batched=True,\n",
    "    remove_columns=[\"text\"]  # Remove text column which won't be needed\n",
    ")\n",
    "\n",
    "# Convert label column to make it compatible with the model\n",
    "tokenized_datasets = tokenized_datasets.map(\n",
    "    lambda examples: {\"labels\": examples[\"label\"]},\n",
    "    remove_columns=[\"label\"]  # Remove original label column\n",
    ")\n",
    "\n",
    "# Check the structure of our processed dataset\n",
    "print(\"\\nSample from processed dataset:\")\n",
    "sample = tokenized_datasets[\"train\"][0]\n",
    "print(f\"Type: {type(sample)}\")\n",
    "print(f\"Keys: {sample.keys()}\")\n",
    "print(f\"Example item: {sample}\")\n",
    "\n",
    "# Define training arguments\n",
    "# Check the TrainingArguments available parameters\n",
    "from inspect import signature\n",
    "print(\"Available parameters for TrainingArguments:\", signature(TrainingArguments))\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    # Try with no evaluation strategy parameter first\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    # Simplified arguments to minimize potential issues\n",
    "    logging_dir=\"./logs\"\n",
    ")\n",
    "\n",
    "# Define trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"test\"],\n",
    ")\n",
    "\n",
    "# Train model\n",
    "print(\"\\nStarting training...\")\n",
    "trainer.train()\n",
    "\n",
    "# Evaluate model\n",
    "print(\"\\nEvaluating model...\")\n",
    "eval_results = trainer.evaluate()\n",
    "print(f\"Evaluation results: {eval_results}\")\n",
    "\n",
    "# Save model\n",
    "model_path = \"./imdb-bert-classifier\"\n",
    "model.save_pretrained(model_path)\n",
    "tokenizer.save_pretrained(model_path)\n",
    "print(f\"\\nModel saved to {model_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05623054-3888-445f-ba9c-7a678188de15",
   "metadata": {},
   "source": [
    "## SQuAD "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d742dd0b-7e54-466a-a290-b522ca96adfb",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "from transformers import BertTokenizerFast, BertForQuestionAnswering\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from datasets import load_dataset\n",
    "\n",
    "\n",
    "model_name = \"bert-base-uncased\"\n",
    "\n",
    "tokenizer = BertTokenizerFast.from_pretrained(model_name)\n",
    "model = BertForQuestionAnswering.from_pretrained(model_name)\n",
    "\n",
    "\n",
    "dataset = load_dataset(\"squad\", split=\"train[:1000]\")\n",
    "dataset = dataset.train_test_split(test_size=0.2)\n",
    "\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    questions = [q.strip() for q in examples[\"question\"]]\n",
    "    contexts = [c.strip() for c in examples[\"context\"]]\n",
    "\n",
    "    inputs = tokenizer(\n",
    "        questions,\n",
    "        contexts,\n",
    "        max_length=384,\n",
    "        truncation=\"only_second\",\n",
    "        padding=\"max_length\",\n",
    "        return_offsets_mapping=True,\n",
    "        return_tensors=\"pt\",\n",
    "    )\n",
    "\n",
    "    start_positions = []\n",
    "    end_positions = []\n",
    "\n",
    "    for i, offsets in enumerate(inputs[\"offset_mapping\"]):\n",
    "        answer = examples[\"answers\"][i]\n",
    "        start_char = answer[\"answer_start\"][0]\n",
    "        end_char = start_char + len(answer[\"text\"][0])\n",
    "\n",
    "        sequence_ids = inputs.sequence_ids(i)\n",
    "\n",
    "        context_start = sequence_ids.index(1)\n",
    "        context_end = len(sequence_ids) - 1 - list(reversed(sequence_ids)).index(1)\n",
    "\n",
    "        offsets = offsets[context_start:context_end+1]\n",
    "        for idx, (start, end) in enumerate(offsets):\n",
    "            if start <= start_char and end >= start_char:\n",
    "                start_positions.append(idx + context_start)\n",
    "            if start <= end_char and end >= end_char:\n",
    "                end_positions.append(idx + context_start)\n",
    "\n",
    "    inputs[\"start_positions\"] = start_positions\n",
    "    inputs[\"end_positions\"] = end_positions\n",
    "    return inputs\n",
    "\n",
    "tokenized_dataset = dataset.map(preprocess_function, batched=True, remove_columns=dataset[\"train\"].column_names)\n",
    "\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./qa-results\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=3e-5,\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    weight_decay=0.01,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset[\"train\"],\n",
    "    eval_dataset=tokenized_dataset[\"test\"],\n",
    ")\n",
    "\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bffa11b9-df02-4a3b-820c-385cbbb3a7a9",
   "metadata": {},
   "source": [
    "# Output\n",
    "\n",
    "## HIDDEN STATE (HIDDEN OUTPUT)\n",
    "\n",
    "## POOLER OUTPUT ([CLS])\n",
    "\n",
    "| Output        | Bentuk                                       | Gunanya                                                                                  |\n",
    "| :------------ | :------------------------------------------- | :--------------------------------------------------------------------------------------- |\n",
    "| Hidden States | `(batch_size, sequence_length, hidden_size)` | Tiap token punya representasi kontekstual. Cocok buat tagging per token (NER, QA).       |\n",
    "| Pooler Output | `(batch_size, hidden_size)`                  | Representasi seluruh kalimat. Cocok buat klasifikasi kalimat (Pos/Neg, Entailment, dst). |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cec4c009-d866-482c-8659-4133982b10f2",
   "metadata": {},
   "source": [
    "# HYPERPARAMETER\n",
    "\n",
    "| Hyperparameter | BERT Umum      | Kenapa              |\n",
    "| -------------- | -------------- | ------------------- |\n",
    "| Learning Rate  | 2e-5 atau 3e-5 | Supaya update halus |\n",
    "| Batch Size     | 8 atau 16      | Supaya hemat memori |\n",
    "| Epoch          | 2-4            | Cukup buat adaptasi |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "930244c6-70a7-4b92-bf33-3f69d9ae271f",
   "metadata": {},
   "source": [
    "# QA MANUALLY\n",
    "\n",
    "- Masukin input âž” dapat logits (start_logits, end_logits).\n",
    "\n",
    "- Cari index paling tinggi dari start_logits âž” itu awal jawaban.\n",
    "\n",
    "- Cari index paling tinggi dari end_logits âž” itu akhir jawaban.\n",
    "\n",
    "- Potong token dari start sampai end âž” decode jadi teks jawaban.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8428e907-a52c-4eaf-ac9a-4515d9dd667e",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/huggingface_hub/file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Answer: \n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizerFast, BertForQuestionAnswering\n",
    "import torch\n",
    "\n",
    "# Load model\n",
    "model_name = \"bert-base-uncased\"\n",
    "tokenizer = BertTokenizerFast.from_pretrained(model_name)\n",
    "model = BertForQuestionAnswering.from_pretrained(model_name)\n",
    "\n",
    "# Input\n",
    "context = \"The capital city of France is Paris.\"\n",
    "question = \"What is the capital of France?\"\n",
    "\n",
    "# Tokenize\n",
    "inputs = tokenizer(question, context, return_tensors=\"pt\")\n",
    "\n",
    "# Forward pass\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "\n",
    "start_logits = outputs.start_logits\n",
    "end_logits = outputs.end_logits\n",
    "\n",
    "# Get the highest scoring start and end token positions\n",
    "start_index = torch.argmax(start_logits)\n",
    "end_index = torch.argmax(end_logits)\n",
    "\n",
    "# Decode the answer\n",
    "answer_ids = inputs[\"input_ids\"][0][start_index : end_index + 1]\n",
    "answer = tokenizer.decode(answer_ids)\n",
    "\n",
    "print(f\"Predicted Answer: {answer}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef91650d-3454-4215-8cc0-6d68bd1fae9e",
   "metadata": {},
   "source": [
    "# MLM (MASKED LANGUAGE MODELLING)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "de9a7ebe-53ff-4cc6-8a93-0b81450dd669",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Token: france\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizerFast, BertForMaskedLM\n",
    "import torch\n",
    "\n",
    "# Load model\n",
    "model_name = \"bert-base-uncased\"\n",
    "tokenizer = BertTokenizerFast.from_pretrained(model_name)\n",
    "model = BertForMaskedLM.from_pretrained(model_name)\n",
    "\n",
    "# Text with mask\n",
    "text = \"The capital of [MASK] is Paris.\"\n",
    "\n",
    "# Tokenize\n",
    "inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "\n",
    "# Forward pass\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "\n",
    "logits = outputs.logits\n",
    "\n",
    "# Index of [MASK]\n",
    "mask_token_index = (inputs.input_ids == tokenizer.mask_token_id)[0].nonzero(as_tuple=True)[0]\n",
    "\n",
    "# Predicted token\n",
    "predicted_token_id = logits[0, mask_token_index].argmax(axis=-1)\n",
    "predicted_token = tokenizer.decode(predicted_token_id)\n",
    "\n",
    "print(f\"Predicted Token: {predicted_token}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cf751d0-21da-4895-8ff5-2095ac98f9a3",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d7f9217e-1c88-4dd1-86d8-9b698b6866f3",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5af0544b-d426-472e-a381-e6ebd2b5f8f2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6dd1831b-700e-4064-be68-c71e22d11444",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1ba11831-60a6-410d-a519-f548d75d8870",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "02634077-8217-4090-9b64-9b0825ba8028",
   "metadata": {},
   "source": [
    "### EMBEDDING LAYER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "22a949d3-3f79-4e70-a045-4bb750d8553f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "eed734ea-d3a7-43c1-908b-de2f235372aa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b5ce1b7d-d0ec-4c55-bd30-1ee038816262",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 3, 10)\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3bf1ff8d-a0a2-4fe0-aaf8-ff2c368ae1a2",
   "metadata": {},
   "source": [
    "### SCALED DOT PRODUCT ATTENTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ce867f88-e010-43a8-9a37-0cc605159019",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b35eb48f-a285-4e9a-b41b-8a5a165e11a6",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 3, 1, 1, 10)\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "941465c8-631a-49b1-b1cc-31ed7383bbc7",
   "metadata": {},
   "source": [
    "### MULTIHEAD ATTENTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4f5c3b85-8eca-4bb1-826f-b44398c03d7a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "0dfd0ec1-2639-4560-9de4-71f4ee7d7c74",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input : [[[-0.77281009 -0.26525163  0.49480591 -0.71234687  1.55256265\n",
      "    0.03180343  0.97385523 -1.85170586]\n",
      "  [-0.57396265 -0.72822389 -1.92776488  1.28525226  0.62958119\n",
      "    0.74690941  0.74743178  0.4630312 ]\n",
      "  [ 0.27997036  0.23979904 -0.09229853 -0.70767435 -0.09800739\n",
      "   -0.34597603  1.06333414  0.25265971]\n",
      "  [-1.82387414  0.24315024 -0.23913714  0.07299795 -1.43867199\n",
      "    0.2218916   0.79692588 -2.53512674]]\n",
      "\n",
      " [[-0.99488304  0.85132085  0.08817293  1.058072    0.68034666\n",
      "   -0.88408008 -0.6120395  -0.78602775]\n",
      "  [ 0.68272199  0.24199436  0.38193769 -0.50565454 -0.26785863\n",
      "    1.48656754 -1.38883187 -0.58510711]\n",
      "  [-0.64977469  0.68919681  1.23141812  0.39152976 -1.00440959\n",
      "   -0.97901076 -1.48389504 -0.96477045]\n",
      "  [ 0.32549657  1.18308707 -0.0607689   1.34716348  0.15218075\n",
      "    0.48560845  0.78811011 -0.05491841]]]\n",
      "\n",
      "shape of input : (2, 4, 8)\n",
      "\n",
      "batch_size = sentences : 2\n",
      "seq_len = words : 4\n",
      "d_model = dimention : 8\n",
      "\n",
      "output : [[[-0.00044888 -0.00099372 -0.00042065 -0.00070045 -0.00093345\n",
      "   -0.00113741 -0.00064498 -0.00043073]\n",
      "  [-0.00044818 -0.0009925  -0.00042007 -0.00069967 -0.0009322\n",
      "   -0.00113622 -0.0006435  -0.00042987]\n",
      "  [-0.00044854 -0.00099315 -0.00042037 -0.00070006 -0.00093289\n",
      "   -0.00113679 -0.00064451 -0.00043034]\n",
      "  [-0.00045097 -0.00099701 -0.00042244 -0.00070298 -0.00093677\n",
      "   -0.00114098 -0.00064773 -0.00043307]]\n",
      "\n",
      " [[-0.00061781 -0.00045114 -0.00040297 -0.00049577 -0.00059227\n",
      "   -0.00043976 -0.00043851 -0.0005988 ]\n",
      "  [-0.00061808 -0.00045135 -0.00040318 -0.00049617 -0.00059253\n",
      "   -0.00043992 -0.00043878 -0.00059887]\n",
      "  [-0.0006196  -0.00045375 -0.00040454 -0.00049788 -0.00059495\n",
      "   -0.00044245 -0.00044084 -0.000601  ]\n",
      "  [-0.00061471 -0.00044659 -0.00040034 -0.0004924  -0.00058749\n",
      "   -0.00043502 -0.00043467 -0.0005948 ]]]\n",
      "\n",
      "expected_shape : (2, 4, 8)\n",
      "\n",
      "actual_shape : (2, 4, 8)\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9799ea6c-b17b-44a3-aed1-d75b3a2e323e",
   "metadata": {},
   "source": [
    "### FFN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2f12ab84-96bc-4686-9a3f-e96e87ae527e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "4c6a4e14-630d-45d3-8d4c-f2fb674949b5",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Input shape: (2, 3, 4)\n",
      "Output shape: (2, 3, 4)\n",
      "Output:\n",
      " [[[24.93228925 25.01672739 28.52430536 27.89781689]\n",
      "  [25.15489125 24.22581311 27.73751544 26.67556067]\n",
      "  [36.99291892 36.90875791 41.84860775 40.34837315]]\n",
      "\n",
      " [[26.06460181 25.73991014 29.46731452 28.66470778]\n",
      "  [35.34297198 34.76383522 39.95592629 39.16061508]\n",
      "  [23.08314383 22.70280699 25.94482227 25.20208626]]]\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8901445b-5339-4fa6-9ce0-101720689695",
   "metadata": {},
   "source": [
    "### STACK ENCODER - 3 LAYER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "eac2a888-ee46-4655-a408-eb75ccf8d80e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "bc4fa2a9-df12-4311-a5e0-214c9077bb3c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "032f81e0-5fdf-4b13-bf9c-b14f6cc1b945",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output shape: (2, 6, 768)\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ee2b9425-a0ec-48f8-af4b-132bbd615405",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1d7f697b-bacb-4add-a8e1-7505e48eda46",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "05f074b9-7ff0-4d44-b23b-e97f45b49293",
   "metadata": {},
   "source": [
    "## NEXT SENTENCE PREDICTION (NSP)\n",
    "\n",
    "- Dikasih dua kalimat, suruh BERT tebak:\n",
    "\n",
    "- Nyambung? (A âž” B)\n",
    "\n",
    "- Atau ngaco? (A âž” random)\n",
    "\n",
    "- Kalimat 1: \"Saya pergi ke pasar.\"\n",
    "- Kalimat 2: \"Saya membeli buah.\"\n",
    "- ==> Label: IsNext (nyambung)\n",
    "\n",
    "- Kalimat 1: \"Saya pergi ke pasar.\"\n",
    "- Kalimat 2: \"Bulan purnama sangat indah.\"\n",
    "- ==> Label: NotNext (acak)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe989b18-41f2-48ee-8f48-394a1e17081d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
