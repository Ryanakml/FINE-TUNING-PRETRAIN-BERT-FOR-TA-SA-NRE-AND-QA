{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "284516a5-86b3-4ebd-954c-c45520507fef",
   "metadata": {},
   "source": [
    "## BERT (Bidirectional Encoder Representation from Transformers)\n",
    "\n",
    "<img src=\"https://miro.medium.com/v2/resize:fit:1162/1*vG_xN7a9HuLCU05U5IznPQ.png\" width=\"500\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbb336c7-edef-4540-b65d-99d73df21e71",
   "metadata": {},
   "source": [
    "- Apa itu Attention (udah kamu kuasai feeling-nya ðŸ”¥)\n",
    "\n",
    "- Bagaimana Attention dihitung (Query-Key-Value + hitungan kecil â†’ lagi kita lakuin sekarang)\n",
    "\n",
    "- Self-Attention di semua kata di kalimat (next step kita)\n",
    "\n",
    "- Multi-Head Attention (kenapa pakai banyak Attention barengan?)\n",
    "\n",
    "- Positional Encoding (gimana BERT tau urutan kata?) -> Learned Positional Embedding.\n",
    "\n",
    "- Encoder Stack (BERT = tumpukan encoder, kayak kue lapis)\n",
    "\n",
    "- Training Objective BERT (Masked Language Model + Next Sentence Prediction)\n",
    "\n",
    "- Fine-Tuning (gimana BERT dipakai buat tugas kayak QA, Sentimen, dst.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd253769-3ec8-46f3-81f5-815df672582f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# SCALED DOT PRODUCT ATTENTION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8777edb-9d53-4232-bc57-1852afaa220f",
   "metadata": {},
   "source": [
    "# FULL STACK ENCODER"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6a08790-0641-4999-8f92-d86c2f37709b",
   "metadata": {},
   "source": [
    "# PRETRAINING \n",
    "\n",
    "## MLM (MASKED LANGUAGE MODELLING)\n",
    "\n",
    "\n",
    "## NSP (NEXT SENTENCE PREDICTION)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ae2badb-d598-4c51-895f-1bede169ad59",
   "metadata": {},
   "source": [
    "# FINE-TUNING\n",
    "\n",
    "## Text Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5140e90-7f05-4e28-bf86-18d9defce5e6",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    BertTokenizer,\n",
    "    BertForSequenceClassification,\n",
    "    TrainingArguments,\n",
    "    Trainer\n",
    ")\n",
    "\n",
    "# Check if GPU is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Load pretrained model and tokenizer\n",
    "model_name = \"bert-base-uncased\"\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "model = BertForSequenceClassification.from_pretrained(model_name, num_labels=2)\n",
    "model.to(device)  # Move model to appropriate device\n",
    "\n",
    "# Load dataset\n",
    "dataset = load_dataset(\"imdb\", split=\"train[:2000]\")\n",
    "dataset = dataset.train_test_split(test_size=0.2)\n",
    "print(f\"Train dataset size: {len(dataset['train'])}\")\n",
    "print(f\"Test dataset size: {len(dataset['test'])}\")\n",
    "\n",
    "# Tokenize function - create a function that properly formats data\n",
    "def tokenize_function(examples):\n",
    "    # Return tokenized examples with proper padding and truncation\n",
    "    return tokenizer(\n",
    "        examples[\"text\"],\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=512,  # Specify max_length explicitly\n",
    "        return_tensors=\"pt\"  # Return PyTorch tensors\n",
    "    )\n",
    "\n",
    "# Process dataset to have the right format and data types\n",
    "tokenized_datasets = dataset.map(\n",
    "    tokenize_function,\n",
    "    batched=True,\n",
    "    remove_columns=[\"text\"]  # Remove text column which won't be needed\n",
    ")\n",
    "\n",
    "# Convert label column to make it compatible with the model\n",
    "tokenized_datasets = tokenized_datasets.map(\n",
    "    lambda examples: {\"labels\": examples[\"label\"]},\n",
    "    remove_columns=[\"label\"]  # Remove original label column\n",
    ")\n",
    "\n",
    "# Check the structure of our processed dataset\n",
    "print(\"\\nSample from processed dataset:\")\n",
    "sample = tokenized_datasets[\"train\"][0]\n",
    "print(f\"Type: {type(sample)}\")\n",
    "print(f\"Keys: {sample.keys()}\")\n",
    "print(f\"Example item: {sample}\")\n",
    "\n",
    "# Define training arguments\n",
    "# Check the TrainingArguments available parameters\n",
    "from inspect import signature\n",
    "print(\"Available parameters for TrainingArguments:\", signature(TrainingArguments))\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    # Try with no evaluation strategy parameter first\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    # Simplified arguments to minimize potential issues\n",
    "    logging_dir=\"./logs\"\n",
    ")\n",
    "\n",
    "# Define trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"test\"],\n",
    ")\n",
    "\n",
    "# Train model\n",
    "print(\"\\nStarting training...\")\n",
    "trainer.train()\n",
    "\n",
    "# Evaluate model\n",
    "print(\"\\nEvaluating model...\")\n",
    "eval_results = trainer.evaluate()\n",
    "print(f\"Evaluation results: {eval_results}\")\n",
    "\n",
    "# Save model\n",
    "model_path = \"./imdb-bert-classifier\"\n",
    "model.save_pretrained(model_path)\n",
    "tokenizer.save_pretrained(model_path)\n",
    "print(f\"\\nModel saved to {model_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05623054-3888-445f-ba9c-7a678188de15",
   "metadata": {},
   "source": [
    "## SQuAD "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d742dd0b-7e54-466a-a290-b522ca96adfb",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "from transformers import BertTokenizerFast, BertForQuestionAnswering\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from datasets import load_dataset\n",
    "\n",
    "\n",
    "model_name = \"bert-base-uncased\"\n",
    "\n",
    "tokenizer = BertTokenizerFast.from_pretrained(model_name)\n",
    "model = BertForQuestionAnswering.from_pretrained(model_name)\n",
    "\n",
    "\n",
    "dataset = load_dataset(\"squad\", split=\"train[:1000]\")\n",
    "dataset = dataset.train_test_split(test_size=0.2)\n",
    "\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    questions = [q.strip() for q in examples[\"question\"]]\n",
    "    contexts = [c.strip() for c in examples[\"context\"]]\n",
    "\n",
    "    inputs = tokenizer(\n",
    "        questions,\n",
    "        contexts,\n",
    "        max_length=384,\n",
    "        truncation=\"only_second\",\n",
    "        padding=\"max_length\",\n",
    "        return_offsets_mapping=True,\n",
    "        return_tensors=\"pt\",\n",
    "    )\n",
    "\n",
    "    start_positions = []\n",
    "    end_positions = []\n",
    "\n",
    "    for i, offsets in enumerate(inputs[\"offset_mapping\"]):\n",
    "        answer = examples[\"answers\"][i]\n",
    "        start_char = answer[\"answer_start\"][0]\n",
    "        end_char = start_char + len(answer[\"text\"][0])\n",
    "\n",
    "        sequence_ids = inputs.sequence_ids(i)\n",
    "\n",
    "        context_start = sequence_ids.index(1)\n",
    "        context_end = len(sequence_ids) - 1 - list(reversed(sequence_ids)).index(1)\n",
    "\n",
    "        offsets = offsets[context_start:context_end+1]\n",
    "        for idx, (start, end) in enumerate(offsets):\n",
    "            if start <= start_char and end >= start_char:\n",
    "                start_positions.append(idx + context_start)\n",
    "            if start <= end_char and end >= end_char:\n",
    "                end_positions.append(idx + context_start)\n",
    "\n",
    "    inputs[\"start_positions\"] = start_positions\n",
    "    inputs[\"end_positions\"] = end_positions\n",
    "    return inputs\n",
    "\n",
    "tokenized_dataset = dataset.map(preprocess_function, batched=True, remove_columns=dataset[\"train\"].column_names)\n",
    "\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./qa-results\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=3e-5,\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    weight_decay=0.01,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset[\"train\"],\n",
    "    eval_dataset=tokenized_dataset[\"test\"],\n",
    ")\n",
    "\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bffa11b9-df02-4a3b-820c-385cbbb3a7a9",
   "metadata": {},
   "source": [
    "# Output\n",
    "\n",
    "## HIDDEN STATE (HIDDEN OUTPUT)\n",
    "\n",
    "## POOLER OUTPUT ([CLS])\n",
    "\n",
    "| Output        | Bentuk                                       | Gunanya                                                                                  |\n",
    "| :------------ | :------------------------------------------- | :--------------------------------------------------------------------------------------- |\n",
    "| Hidden States | `(batch_size, sequence_length, hidden_size)` | Tiap token punya representasi kontekstual. Cocok buat tagging per token (NER, QA).       |\n",
    "| Pooler Output | `(batch_size, hidden_size)`                  | Representasi seluruh kalimat. Cocok buat klasifikasi kalimat (Pos/Neg, Entailment, dst). |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cec4c009-d866-482c-8659-4133982b10f2",
   "metadata": {},
   "source": [
    "# HYPERPARAMETER\n",
    "\n",
    "| Hyperparameter | BERT Umum      | Kenapa              |\n",
    "| -------------- | -------------- | ------------------- |\n",
    "| Learning Rate  | 2e-5 atau 3e-5 | Supaya update halus |\n",
    "| Batch Size     | 8 atau 16      | Supaya hemat memori |\n",
    "| Epoch          | 2-4            | Cukup buat adaptasi |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "930244c6-70a7-4b92-bf33-3f69d9ae271f",
   "metadata": {},
   "source": [
    "# QA MANUALLY\n",
    "\n",
    "- Masukin input âž” dapat logits (start_logits, end_logits).\n",
    "\n",
    "- Cari index paling tinggi dari start_logits âž” itu awal jawaban.\n",
    "\n",
    "- Cari index paling tinggi dari end_logits âž” itu akhir jawaban.\n",
    "\n",
    "- Potong token dari start sampai end âž” decode jadi teks jawaban.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8428e907-a52c-4eaf-ac9a-4515d9dd667e",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/huggingface_hub/file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Answer: \n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizerFast, BertForQuestionAnswering\n",
    "import torch\n",
    "\n",
    "# Load model\n",
    "model_name = \"bert-base-uncased\"\n",
    "tokenizer = BertTokenizerFast.from_pretrained(model_name)\n",
    "model = BertForQuestionAnswering.from_pretrained(model_name)\n",
    "\n",
    "# Input\n",
    "context = \"The capital city of France is Paris.\"\n",
    "question = \"What is the capital of France?\"\n",
    "\n",
    "# Tokenize\n",
    "inputs = tokenizer(question, context, return_tensors=\"pt\")\n",
    "\n",
    "# Forward pass\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "\n",
    "start_logits = outputs.start_logits\n",
    "end_logits = outputs.end_logits\n",
    "\n",
    "# Get the highest scoring start and end token positions\n",
    "start_index = torch.argmax(start_logits)\n",
    "end_index = torch.argmax(end_logits)\n",
    "\n",
    "# Decode the answer\n",
    "answer_ids = inputs[\"input_ids\"][0][start_index : end_index + 1]\n",
    "answer = tokenizer.decode(answer_ids)\n",
    "\n",
    "print(f\"Predicted Answer: {answer}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef91650d-3454-4215-8cc0-6d68bd1fae9e",
   "metadata": {},
   "source": [
    "# MLM (MASKED LANGUAGE MODELLING)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "de9a7ebe-53ff-4cc6-8a93-0b81450dd669",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Token: france\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizerFast, BertForMaskedLM\n",
    "import torch\n",
    "\n",
    "# Load model\n",
    "model_name = \"bert-base-uncased\"\n",
    "tokenizer = BertTokenizerFast.from_pretrained(model_name)\n",
    "model = BertForMaskedLM.from_pretrained(model_name)\n",
    "\n",
    "# Text with mask\n",
    "text = \"The capital of [MASK] is Paris.\"\n",
    "\n",
    "# Tokenize\n",
    "inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "\n",
    "# Forward pass\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "\n",
    "logits = outputs.logits\n",
    "\n",
    "# Index of [MASK]\n",
    "mask_token_index = (inputs.input_ids == tokenizer.mask_token_id)[0].nonzero(as_tuple=True)[0]\n",
    "\n",
    "# Predicted token\n",
    "predicted_token_id = logits[0, mask_token_index].argmax(axis=-1)\n",
    "predicted_token = tokenizer.decode(predicted_token_id)\n",
    "\n",
    "print(f\"Predicted Token: {predicted_token}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe989b18-41f2-48ee-8f48-394a1e17081d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
