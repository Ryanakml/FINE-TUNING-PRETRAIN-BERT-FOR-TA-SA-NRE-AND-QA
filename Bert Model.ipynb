{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f30eb8b4-7a3e-4663-9678-257600400e21",
   "metadata": {},
   "source": [
    "# MODEL : MINI BERT MANUALLY\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## PROCESS OVERVIEW\n",
    "\n",
    "| Step | Penjelasan                           | Status |\n",
    "| :--: | :----------------------------------- | :----: |\n",
    "|   1  | Bangun Mini-BERT Stack               |   Now   |\n",
    "|   2  | Pretraining (Masked LM + NSP)        |   ðŸ”œ   |\n",
    "|   3  | Fine-tuning ke task spesifik         |   ðŸ”œ   |\n",
    "|   4  | Buat dataset dummy buat latihan      |   ðŸ”œ   |\n",
    "|   5  | Build mindset & intuition level dewa |   ðŸ”œ   |\n",
    "\n",
    "---\n",
    "\n",
    "## ARCHITECTURE\n",
    "\n",
    "<img src=\"https://miro.medium.com/v2/resize:fit:1162/1*vG_xN7a9HuLCU05U5IznPQ.png\" width=\"500\" />\n",
    "\n",
    "\n",
    "## INPUT DATA\n",
    "\n",
    "## TOKENIZATION\n",
    "\n",
    "- Transforming every word into token (int ID) using vocab\n",
    "\n",
    "- WordPiece for BERT, for example :\n",
    "\n",
    "- Kalimat: \"cat eating fish\"\n",
    "- Tokenized â†’ [11, 25, 38]\n",
    "- Vocab â†’ {1 : 11, 2 : 25, 3 : 38}\n",
    "\n",
    "\n",
    "## POSITIONAL EMBEDDINGS\n",
    "\n",
    "- every token will turn into vector representation + its position.\n",
    "\n",
    "- step :\n",
    "\n",
    "- word embedding lookup table n dimentional for every word\n",
    "\n",
    "- positional encoding lookup table for every position word and add with embedding value.\n",
    "\n",
    "- example :\n",
    "\n",
    "- Word embed [128-d]\n",
    "\n",
    "- Position embed [128-d]\n",
    "\n",
    "- Add embed-position â†’ we get [128-d] for every token.\n",
    "\n",
    "\n",
    "\n",
    "## STACK ENCODER\n",
    "\n",
    "#### MULTIHEAD ATTENTION\n",
    "\n",
    "- Multi head meaning every word has n perpective on how that word context is.\n",
    "\n",
    "##### SCALED DOT PRODUCT ATTENTION / SELF ATTENTION\n",
    "\n",
    "- every token look into another token and update its representation based on proporsition telling relationship.\n",
    "\n",
    "\n",
    "- Step :\n",
    "\n",
    "- Input Vector â†’ Linear projection to Q, K, V.\n",
    "\n",
    "- attention score softmax(QKáµ€ / âˆšd).\n",
    "\n",
    "- mutiply score Ã— V.\n",
    "\n",
    "- Concacinate with another head\n",
    "\n",
    "- Output: new hidden state that more \"smart\" - has more context.\n",
    "\n",
    "\n",
    "#### RESIDUAL AND NORMALIZATION\n",
    "\n",
    "- jaga info lama + stabilkan training.\n",
    "- Tambahkan input + output Self-Attention âž” Residual.\n",
    "- Normalisasi hasilnya âž” LayerNorm.\n",
    "- attention_output = LayerNorm(input + attention(input))\n",
    "\n",
    "\n",
    "#### FEED FORWARD\n",
    "\n",
    "- Perbaiki representasi token secara individual.\n",
    "\n",
    "- proses :\n",
    "\n",
    "- expand â†’ relu â†’ shrink.\n",
    "\n",
    "- Seleksi fitur-fitur penting per token.\n",
    "\n",
    "- detail :\n",
    "\n",
    "- Dense layer 1: expand dimensi (misal 128 âž” 512).\n",
    "\n",
    "- ReLU : buang informasi ga guna, fokus ke internal connection\n",
    "\n",
    "- Dense layer 2: balik ke dimensi awal (512 âž” 128).\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#### RESIDUAL AND NORMALIZATION\n",
    "\n",
    "- jaga info lama + stabilkan training.\n",
    "- Tambahkan input + output Self-Attention âž” Residual.\n",
    "- Normalisasi hasilnya âž” LayerNorm.\n",
    "- attention_output = LayerNorm(input + attention(input))\n",
    "- ffn_output = LayerNorm(attention_output + FFN(attention_output))\n",
    "\n",
    "\n",
    "## ANOTHER ENCODER BLOCK\n",
    "\n",
    "- input = ffn_output\n",
    "\n",
    "- ....\n",
    "\n",
    "-  ......\n",
    "\n",
    "- 12 - 24X\n",
    "\n",
    "## OUTPUT \n",
    "\n",
    "\n",
    "### HIDDEN STATE\n",
    "\n",
    "- Hidden States\t(batch_size, sequence_length, hidden_size)\n",
    "- Tiap token punya representasi kontekstual.\n",
    "- buat tagging per token (NER, QA).\n",
    "\n",
    "\n",
    "### POOLED STATE\n",
    "\n",
    "- Pooler Output (batch_size, hidden_size)\n",
    "- Representasi seluruh kalimat.\n",
    "- buat klasifikasi kalimat (Pos/Neg, Entailment, dst)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a52a669-aba3-4c3a-80a5-74a0b5f7925e",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## PSEUDOCODE INPLEMENTATION\n",
    "\n",
    "### INPUT\n",
    "    ## input text or corpus\n",
    "    \n",
    "    c = \"x y z ....\"\n",
    "\n",
    "### TOKEN\n",
    "\n",
    "    # text into number by id or index\n",
    "    \n",
    "    t = [x_id, y_id, z_id, ...... , n_id]\n",
    "\n",
    "### POSITIONAL EMBEDDING\n",
    "\n",
    "    ## encode each token to embed and position\n",
    "    \n",
    "    for each token in t:\n",
    "    \n",
    "    embedding vector = word embedding matrix [token_id] -> vectorize the input token for each word\n",
    "    positional vector = positional embeedin matrix [position] -> vector that tell what the order of each token by poition\n",
    "    input that vectorized: input_vector = embedding vector + positional vector\n",
    "\n",
    "    ## result:\n",
    "    \n",
    "    vectors  = [input_vector_1, input_vector_2, input_vector_3, ......., input_vector_n]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### STACK ENCODER - 2 LAYER\n",
    "\n",
    "    ## for each encoder layer :\n",
    "\n",
    "        ## if multihead self attention :\n",
    "    \n",
    "        for each token in t :\n",
    "            linear projection 1, 2, 3 ...., n =  Q, K, V  from input vectors\n",
    "        \n",
    "        for each pair of token (i,j) : \n",
    "            attention score 1, 2, 3 ...., n = dot product (Q[i], K[j]) / sqrt (d_model = input vectors [-1])\n",
    "\n",
    "        for each token i:\n",
    "            normalize the value by softmax accross attention score\n",
    "            attention output 1, 2, 3 ...., n= sum over j (softmax_score[i][j] * V[j])\n",
    "\n",
    "            \n",
    "        ## if self attention block\n",
    "\n",
    "        for each token in t :\n",
    "            linear projection =  Q, K, V  from input vectors\n",
    "        \n",
    "        for each pair of token (i,j) : \n",
    "            attention score = dot product (Q[i], K[j]) / sqrt (d_model = input vectors [-1])\n",
    "\n",
    "        for each token i:\n",
    "            normalize the value by softmax accross attention score\n",
    "            attention output = sum over j (softmax_score[i][j] * V[j])\n",
    "\n",
    "\n",
    "        ## residual connection \n",
    "    \n",
    "        attention output = layer_norm(input + attention output)\n",
    "\n",
    "\n",
    "        ## feed forward \n",
    "        \n",
    "        for each token in attention output :\n",
    "            - expand dimention 2x : h1 : linear(attention output)\n",
    "            - focus on important information in that token only : relu : relu(h1)\n",
    "            - compress again 2x -> original : h2 / output : linear(relu)\n",
    "\n",
    "\n",
    "        ## residual connection \n",
    "    \n",
    "        output = layer_norm(attention output + output)\n",
    "\n",
    "    \n",
    "        ## Output\n",
    "    \n",
    "        input vetor in the next encoder layer - repeat the process from linear projection - output.\n",
    "\n",
    "\n",
    "### OUTPUT - STATE\n",
    "\n",
    "    ## output hidden state\n",
    "    \n",
    "    Return:\n",
    "    - Output hidden vectors for each token\n",
    "    \n",
    "    ## output pooled state\n",
    "    \n",
    "    Return:\n",
    "    - [CLS] token output for classification tasks (if needed)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43c06402-a2e4-4959-b001-a8c32016dc78",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## SIMPLE PYTHON IMPLEMENTATION\n",
    "\n",
    "### STRUCTURE\n",
    "\n",
    "\n",
    "| Puzzle | Modul           | Isi                                                                                          |\n",
    "| :----: | :-------------- | :------------------------------------------------------------------------------------------- |\n",
    "|    1   | Basic Functions | - Matrix multiplication (manual) <br> - Softmax Activation Function (manual) <br> - Layer normalization (manual) |\n",
    "|    2   | Embedding       | - Word Embedding Lookup table <br> - Positional Encoding Lookup table                                     |\n",
    "|    3   | Self Attention  | - Linear projection (Q, K, V) of input <br> - Attention score calculation <br> - Output V             |\n",
    "|    4   | Feed Forward    | - Linear 1 (expand), ReLU, Linear 2 (compress again)                                                |\n",
    "|    5   | Encoder Layer   | - Residual connection + Norm after Attention <br> - Residual connection + Norm after FFN |\n",
    "\n",
    "\n",
    "### BASIC FUNCTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fc62ae59-b715-4611-b8ec-cae5cbc88866",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dot Product Function\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def matmul(x,y): \n",
    "    \n",
    "    return np.dot(x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fa854918-df1b-4953-a042-f1ade765e7c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Softmax Activation Funtion\n",
    "\n",
    "def softmax(x, axis=-1):\n",
    "    x_max = np.max(x, axis=axis, keepdims=True)\n",
    "    e_x = np.exp(x - x_max)\n",
    "    \n",
    "    return e_x / np.sum(e_x, axis=axis, keepdims=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "26b665a6-090f-42c5-8b85-bf70acc1d777",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalization Layer Function\n",
    "\n",
    "def norm(x):\n",
    "    \n",
    "    eps = 1e-6\n",
    "    avg = np.mean(x, axis=-1, keepdims=True)\n",
    "    std = np.std(x, axis=-1, keepdims=True)\n",
    "\n",
    "    return (x - avg)/(std + eps)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e4c83a3-df29-41bd-a0a2-a7ba8949b1be",
   "metadata": {},
   "source": [
    "### EMBEDDING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5f831ddb-d08d-4806-9183-82218a4f6e85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word Embedding Lookup Table\n",
    "\n",
    "class wordEmbedding:\n",
    "    def __init__(self, vocab_size, d_model):\n",
    "        self.vocab_size = vocab_size\n",
    "        self.d_model = d_model\n",
    "        self.embedding_table = np.random.rand(vocab_size, d_model)*0.01\n",
    "\n",
    "    def forward(self, token_id):\n",
    "        return self.embedding_table[token_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2b64a37a-b31e-4379-8433-bf3d8c02c2f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Positional Encoding Lookup table\n",
    "\n",
    "class positionalEncoding:\n",
    "    def __init__(self, seq_len, d_model):\n",
    "        self.seq_len = seq_len\n",
    "        self.d_model = d_model\n",
    "\n",
    "        position = np.arange(seq_len)[:, np.newaxis]\n",
    "        div_term = np.exp(np.arange(0, d_model, 2) * (-np.log(10000.0) / d_model))\n",
    "\n",
    "        pe = np.zeros((seq_len, d_model))\n",
    "        pe[:, 0::2] = np.sin(position * div_term)\n",
    "        pe[:, 1::2] = np.cos(position * div_term)\n",
    "\n",
    "        self.embedding_table = pe\n",
    "\n",
    "    def forward(self, position_token_id):\n",
    "        return self.embedding_table[position_token_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "46b3fa15-18d2-4a1d-a8af-cc3262ce7548",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 3, 10)\n"
     ]
    }
   ],
   "source": [
    "# EMBEDDING XAMPLE\n",
    "\n",
    "vocab_size = 100\n",
    "d_model = 10\n",
    "max_seq = 5\n",
    "\n",
    "word_embed = wordEmbedding(vocab_size, d_model)\n",
    "position_embed = positionalEncoding(max_seq, d_model)\n",
    "\n",
    "corpus = [['i', 'like', 'pussies'], ['he', 'hate', 'dogs']]\n",
    "\n",
    "tokens = np.array([\n",
    "    [5, 8, 20],   # Sentence 1\n",
    "    [6, 9, 25]    # Sentence 2\n",
    "])  # shape: (2, 3)\n",
    "\n",
    "positions = np.array([0, 1, 2])  # for all sentences, same positions\n",
    "\n",
    "\n",
    "word_vecs = word_embed.forward(tokens)\n",
    "pos_vecs = position_embed.forward(positions)\n",
    "\n",
    "final_input = word_vecs + pos_vecs\n",
    "print(final_input.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "680143a0-3ffe-4d17-a754-3260d41ea61b",
   "metadata": {},
   "source": [
    "### SCALED DOT PRODUCT ATTENTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "952e3b9a-f051-43ba-99ad-3932af719823",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear projection (Q, K, V)\n",
    "# Attention score calculation\n",
    "# Output V\n",
    "\n",
    "class selfAttention:\n",
    "    def __init__(self, d_model):\n",
    "        self.d_model = d_model\n",
    "\n",
    "        scale = np.sqrt(2.0 / d_model)\n",
    "        self.w_q = np.random.rand(d_model, d_model) * scale\n",
    "        self.w_k = np.random.rand(d_model, d_model) * scale\n",
    "        self.w_v = np.random.rand(d_model, d_model) * scale\n",
    "\n",
    "    def forward(self, x):\n",
    "        q = matmul(x, self.w_q)\n",
    "        k = matmul(x, self.w_k)\n",
    "        v = matmul(x, self.w_v)\n",
    "\n",
    "        k_t = np.transpose(k, (0, 2, 1))\n",
    "\n",
    "        attn_scores = matmul(q, k_t)/np.sqrt(self.d_model)\n",
    "        attn_probs = softmax(attn_scores, axis=-1)\n",
    "        attn_output = matmul(attn_probs, v)\n",
    "\n",
    "        return attn_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c1a9d058-485a-43c6-aa62-7bc7b7e8cfa6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 3, 1, 1, 10)\n"
     ]
    }
   ],
   "source": [
    "# SCALED DOT PRODUCT ATTENTION EXAMPLE : 3 WORD, 10 DIMENTION\n",
    "\n",
    "x = np.random.randn(1, 3, 10)\n",
    "\n",
    "self_attn = selfAttention(d_model=10)\n",
    "\n",
    "out = self_attn.forward(x)\n",
    "\n",
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e62dd56-bd3a-46cd-8935-ad9d1428013e",
   "metadata": {},
   "source": [
    "### MULTIHEAD ATTENTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9c28bd3f-e729-44a8-9d8a-090047ba674f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multihead Attention\n",
    "\n",
    "class multiheadAttention():\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.heads_d = d_model // num_heads\n",
    "\n",
    "        assert d_model % num_heads == 0\n",
    "\n",
    "        # weigth Q, K, V\n",
    "        scale = np.sqrt(2.0 / d_model)\n",
    "        self.w_q = np.random.rand(d_model, d_model) * scale\n",
    "        self.w_k = np.random.rand(d_model, d_model) * scale\n",
    "        self.w_v = np.random.rand(d_model, d_model) * scale\n",
    "        self.w_o = np.random.rand(d_model, d_model) * scale\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, seq_len, _ = x.shape\n",
    "\n",
    "        # Q, K, V\n",
    "        q = np.matmul(x, self.w_q)\n",
    "        k = np.matmul(x, self.w_k)\n",
    "        v = np.matmul(x, self.w_v)\n",
    "\n",
    "        # split into multiple head\n",
    "        q = q.reshape(batch_size, seq_len, self.num_heads, self.heads_d)\n",
    "        k = k.reshape(batch_size, seq_len, self.num_heads, self.heads_d)\n",
    "        v = v.reshape(batch_size, seq_len, self.num_heads, self.heads_d)\n",
    "\n",
    "        # transpose\n",
    "        q = q.transpose(0,2,1,3)\n",
    "        k = k.transpose(0,2,1,3)\n",
    "        v = v.transpose(0,2,1,3)\n",
    "\n",
    "        k_t = np.transpose(k, (0, 1, 3, 2))\n",
    "\n",
    "        # self attention per head\n",
    "        attn_scores = np.matmul(q, k_t) / np.sqrt(self.heads_d)\n",
    "        attn_probs = softmax(attn_scores, axis=-1)\n",
    "        attn_output = np.matmul(attn_probs, v)\n",
    "\n",
    "        # concatinate all heads into one\n",
    "        attn_output = attn_output.transpose(0,2,1,3)\n",
    "        attn_output = attn_output.reshape(batch_size, seq_len, self.d_model)\n",
    "\n",
    "        # linear projection \n",
    "        output = np.matmul(attn_output, self.w_o)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4e4b14ea-82b4-434a-aae1-fcbe4567c120",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input : [[[ 0.36672737 -0.20878601 -0.93312864  0.55049618  0.61309127\n",
      "    0.59738567  0.97634232  0.46390877]\n",
      "  [ 1.54528626 -1.18411235 -0.50366323  0.55005004 -0.86921243\n",
      "    1.8964261  -1.41468836 -0.15893344]\n",
      "  [-2.1778084   0.19036908 -0.02444217  1.12669448 -0.41743285\n",
      "    0.76707763  0.87854491  0.32731391]\n",
      "  [-0.0439196   1.21966191 -0.31403431 -0.87529854  1.49784223\n",
      "   -0.04638034  0.2728852  -0.75194285]]\n",
      "\n",
      " [[-0.54282411 -0.53131012  0.04423507 -0.76264487  0.31901532\n",
      "    0.4806683   0.16869729  1.01144944]\n",
      "  [-3.65499794 -0.03117673 -1.67538619  0.06720851  1.30975063\n",
      "   -0.60280177  0.55697473 -0.72618966]\n",
      "  [ 0.38107161  0.59713013  0.06308     0.26364099  1.51547201\n",
      "    2.52786876  0.05574878 -1.2881275 ]\n",
      "  [ 1.12629148  0.62244861 -0.3588689   0.53430038  0.33699067\n",
      "    1.57690903 -0.27540016  0.14639957]]]\n",
      "\n",
      "shape of input : (2, 4, 8)\n",
      "\n",
      "batch_size = sentences : 2\n",
      "seq_len = words : 4\n",
      "d_model = dimention : 8\n",
      "\n",
      "output : [[[ 0.39168056  0.58393131  0.67286276  0.47773785  0.67752337\n",
      "    0.54958616  0.5382172   0.22029586]\n",
      "  [ 0.29374484  0.51721946  0.59576791  0.38449357  0.57631254\n",
      "    0.43277663  0.48065967  0.20581307]\n",
      "  [ 0.33921794  0.51255519  0.57668838  0.4084325   0.60892203\n",
      "    0.45002682  0.46834974  0.19492263]\n",
      "  [ 0.34330416  0.54095119  0.63292737  0.44163377  0.60915515\n",
      "    0.49777464  0.5095138   0.2072075 ]]\n",
      "\n",
      " [[ 0.35607504  0.46132369  0.57791098  0.48966051  0.47801378\n",
      "    0.45662589  0.49433249  0.18984734]\n",
      "  [-0.91815572 -1.59194096 -1.79975767 -0.97155256 -1.38466623\n",
      "   -1.70579174 -1.43729525 -0.54997074]\n",
      "  [ 0.97000804  1.70744333  2.07538117  1.32809282  1.70928631\n",
      "    1.63463065  1.67326881  0.67439622]\n",
      "  [ 0.89955994  1.64396019  2.01279277  1.27358089  1.6234897\n",
      "    1.58147866  1.60998139  0.64928831]]]\n",
      "\n",
      "expected_shape : (2, 4, 8)\n",
      "\n",
      "actual_shape : (2, 4, 8)\n"
     ]
    }
   ],
   "source": [
    "# MULTIHEAD XAMPLE : sentences : 2, words : 4, d_model : 8, num of heads = 4 (4 konteks each word)\n",
    "\n",
    "batch_size = 2\n",
    "seq_len = 4\n",
    "d_model = 8\n",
    "num_heads = 4\n",
    "\n",
    "x = np.random.randn(batch_size, seq_len, d_model)  # (2, 4, 8)\n",
    "\n",
    "mha = multiheadAttention(d_model, num_heads)\n",
    "\n",
    "output = mha.forward(x)\n",
    "\n",
    "print(f\"input : {x}\")\n",
    "print()\n",
    "print(f\"shape of input : {x.shape}\")\n",
    "print()\n",
    "\n",
    "batch_size, seq_len, d_model = x.shape\n",
    "\n",
    "print(f\"batch_size = sentences : {batch_size}\")\n",
    "print(f\"seq_len = words : {seq_len}\")\n",
    "print(f\"d_model = dimention : {d_model}\")\n",
    "\n",
    "\n",
    "expected_shape = (batch_size, seq_len, d_model)\n",
    "actual_shape = output.shape\n",
    "\n",
    "print()\n",
    "print(f\"output : {output}\")\n",
    "print()\n",
    "print(f\"expected_shape : {expected_shape}\")\n",
    "print()\n",
    "\n",
    "print(f\"actual_shape : {actual_shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d557941b-a1b3-4784-95a9-c5abc761ae39",
   "metadata": {},
   "source": [
    "### FEED FORWARD NEURAL NETWORKS (FFN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3e414b55-1866-4664-8841-7586ba4104c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feed Forward Neural Network\n",
    "\n",
    "class feedForward:\n",
    "    def __init__(self, d_model, d_ff):\n",
    "        scale1 = np.sqrt(2.0 / d_model)\n",
    "        scale2 = np.sqrt(2.0 / d_ff)\n",
    "        \n",
    "        self.w1 = np.random.rand(d_model, d_ff) * scale1\n",
    "        self.b1 = np.zeros((d_ff,))\n",
    "        self.w2 = np.random.rand(d_ff, d_model) * scale2\n",
    "        self.b2 = np.zeros((d_model,))\n",
    "\n",
    "\n",
    "    def gelu(self, x):\n",
    "        return 0.5 * x * (1 + np.tanh(np.sqrt(2 / np.pi) * (x + 0.044715 * np.power(x, 3))))\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        x_expanded = np.matmul(x, self.w1) + self.b1\n",
    "        x_relu = self.gelu(x_expanded)\n",
    "        output = np.matmul(x_relu, self.w2) + self.b2\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bf524e06-1153-47d9-9ea4-c49bb2e771ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Input shape: (2, 3, 4)\n",
      "Output shape: (2, 3, 4)\n",
      "Output:\n",
      " [[[4.20066293 4.05456212 4.08290323 3.76315567]\n",
      "  [2.23218866 2.17856967 2.21050946 2.08578322]\n",
      "  [4.49221826 4.23657331 4.28514247 3.9508369 ]]\n",
      "\n",
      " [[1.1587879  1.06382888 1.06665058 0.97644287]\n",
      "  [2.81090316 2.74204152 2.77760206 2.52129341]\n",
      "  [3.87867987 3.65292627 3.68881106 3.43366131]]]\n"
     ]
    }
   ],
   "source": [
    "# FFN XMAPLE\n",
    "\n",
    "d_model = 4\n",
    "d_ff = 64\n",
    "batch_size = 2\n",
    "seq_len = 3\n",
    "\n",
    "# dummy input (batch_size, seq_len, d_model)\n",
    "x = np.random.rand(batch_size, seq_len, d_model)\n",
    "\n",
    "# Inisialisasi Feed Forward\n",
    "ff = feedForward(d_model, d_ff)\n",
    "\n",
    "# Proses forward tiap token\n",
    "out = np.array([[ff.forward(token) for token in sample] for sample in x])\n",
    "    \n",
    "print()\n",
    "print(\"Input shape:\", x.shape)\n",
    "print(\"Output shape:\", out.shape)\n",
    "print(\"Output:\\n\", out)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "891c444b-bcfe-4a78-a694-4e33098183fc",
   "metadata": {},
   "source": [
    "### MULTI-STACKS ENCODER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5bcbbb0e-f07d-4c01-9cee-36508f8be95f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STACK / BLOCK BERT ENCODER\n",
    "\n",
    "class bertBlock:\n",
    "    def __init__(self, d_model, num_heads, d_ff):\n",
    "        self.mha = multiheadAttention(d_model, num_heads)\n",
    "        self.hh = feedForward(d_model, d_ff)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # multihead self attention\n",
    "        x_mha = self.mha.forward(x)\n",
    "\n",
    "        # add and norm\n",
    "        x_norm1 = norm(x + x_mha)\n",
    "\n",
    "        # feed forward\n",
    "        x_ffn = self.hh.forward(x_norm1) \n",
    "\n",
    "        # add and norm\n",
    "        x_norm2 = norm(x_norm1 + x_ffn)\n",
    "\n",
    "        return x_norm2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f1b81bda-dc85-4fc6-9fa6-5cf3bf38595c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MULTI STACKS / BLOCK BERT ENCODER MODEL\n",
    "\n",
    "class bertModel:\n",
    "    def __init__(self, vocab_size, seq_len, d_model, num_heads, d_ff, n_stacks):\n",
    "        self.word_embedding = wordEmbedding(vocab_size, d_model)\n",
    "        self.positional_embedding = positionalEncoding(seq_len, d_model)\n",
    "        self.bert_blocks = [bertBlock(d_model, num_heads, d_ff) for _ in range(n_stacks)]\n",
    "\n",
    "    def forward(self, token_id, position_token_id=None):\n",
    "        batch_size, seq_len = token_id.shape\n",
    "\n",
    "        if position_token_id is None:\n",
    "            position_token_id = np.tile(np.arange(seq_len), (batch_size, 1))\n",
    "\n",
    "        word_embedding = self.word_embedding.forward(token_id)\n",
    "        positional_embedding = self.positional_embedding.forward(position_token_id)\n",
    "\n",
    "        x = word_embedding + positional_embedding\n",
    "\n",
    "        for block in self.bert_blocks:\n",
    "            x = block.forward(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "002cf1bb-541c-4110-ac02-fe3f51cde54e",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### USING MODEL XAMPLE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "13819e24-96ba-45c4-98a4-4cf23063d6ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output shape: (2, 6, 768)\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameters\n",
    "vocab_size = 3000\n",
    "seq_len = 512\n",
    "d_model = 768\n",
    "num_heads = 12\n",
    "d_ff = 3072\n",
    "n_stacks = 12\n",
    "    \n",
    "# Create model instance\n",
    "model = bertModel(vocab_size, seq_len, d_model, num_heads, d_ff, n_stacks)\n",
    "    \n",
    "# Example input (batch_size=2, seq_len=6)\n",
    "token_id = np.array([[1, 2, 3, 4, 5, 6], [7, 8, 9, 10, 11, 12]])\n",
    "    \n",
    "# Forward pass\n",
    "output = model.forward(token_id)\n",
    "print(f\"Output shape: {output.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b05eb910-2ce3-409b-84ab-fa5224873c3b",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## PROCESS OVERVIEW\n",
    "\n",
    "| Step | Penjelasan                           | Status |\n",
    "| :--: | :----------------------------------- | :----: |\n",
    "|   1  | Bangun Mini-BERT Stack               |    âœ…   |\n",
    "|   2  | Pretraining (Masked LM + NSP)        |   ðŸ”œ   |\n",
    "|   3  | Fine-tuning ke task spesifik         |   ðŸ”œ   |\n",
    "|   4  | Buat dataset dummy buat latihan      |   ðŸ”œ   |\n",
    "|   5  | Build mindset & intuition level dewa |   ðŸ”œ   |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "189a2e96-0c8b-4954-8ba5-21c8c19db8a8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
