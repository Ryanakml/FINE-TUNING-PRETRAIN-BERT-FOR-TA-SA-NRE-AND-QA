{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "48cde9f7-cf94-4c69-8b34-a5f72c58024e",
   "metadata": {},
   "source": [
    "# Pretraining (Masked LM + NSP)\n",
    "\n",
    "## PROCESS OVERVIEW\n",
    "\n",
    "\n",
    "\n",
    "| Step | Penjelasan                           | Status |\n",
    "| :--: | :----------------------------------- | :----: |\n",
    "|   1  | Bangun Mini-BERT Stack               |    âœ…   |\n",
    "|   2  | Pretraining (Masked LM + NSP)        |   NOW   |\n",
    "|   3  | Fine-tuning ke task spesifik         |   ðŸ”œ   |\n",
    "|   4  | Buat dataset dummy buat latihan      |   ðŸ”œ   |\n",
    "|   5  | Build mindset & intuition level dewa |   ðŸ”œ   |\n",
    "\n",
    "---\n",
    "\n",
    "INTI :\n",
    "\n",
    "- Input: Token yang di-mask sebagian + sepasang kalimat\n",
    "- Target 1: Isi kata yang di-mask\n",
    "- Target 2: Apakah kalimat kedua nyambung?\n",
    "\n",
    "HOW? :\n",
    "\n",
    "- Tokenisasi kalimat âž” jadi token ID\n",
    "\n",
    "- Tambahin [CLS] di awal, [SEP] antar kalimat\n",
    "\n",
    "- Tambahin Positional Encoding kayak biasa\n",
    "\n",
    "- Random pilih token buat di-[MASK] (sekitar 15% token)\n",
    "\n",
    "- Masukin ke Mini-BERT stack - model kita\n",
    "\n",
    "- Output 1: Prediksi isi token yang ketutup\n",
    "\n",
    "- Output 2: Prediksi label NSP (IsNext / NotNext)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "| Misi                              | Tujuan                         | Gampangnya                                     |\n",
    "| :-------------------------------- | :----------------------------- | :--------------------------------------------- |\n",
    "| 1. Masked Language Model (MLM)    | Belajar isi kata yang hilang   | Tebak kata yang ketutupan                      |\n",
    "| 2. Next Sentence Prediction (NSP) | Belajar hubungan antar kalimat | Tebak apakah kalimat kedua nyambung atau ngaco |\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a45eea8-0591-4067-b90d-c1141e3d5614",
   "metadata": {},
   "source": [
    "# MLM : MASK LANGUAGE MODEL\n",
    "\n",
    "\n",
    "## INTUITION\n",
    "\n",
    "- Belajar isi kata yang hilang, tutup beberapa kata dalam kalimat\n",
    "- Tebak kata yang ketutupan , suruh bert nebak itu\n",
    "- Kalimat asli:\n",
    "- \"Saya makan nasi di warung.\"\n",
    "\n",
    "- Setelah masking:\n",
    "- \"Saya [MASK] nasi di [MASK].\"\n",
    "\n",
    "- Tugas BERT:\n",
    "- Tebak [MASK] = \"makan\", [MASK] = \"warung\"\n",
    "\n",
    "\n",
    "## PROCESS\n",
    "\n",
    "1. Input :\n",
    "\n",
    "- c = ['kucing bermain di taman']\n",
    "\n",
    "- t = ['kucing', 'bermain', 'di', 'taman']\n",
    "\n",
    "\n",
    "2. Special Token :\n",
    "\n",
    "- ['[CLS]', 'kucing', 'bermain', 'di', 'taman', '[SEP]']\n",
    "\n",
    "\n",
    "3. Masking 15% Input :\n",
    "\n",
    "- ['[CLS]', 'kucing', '[MASK]', 'di', 'taman', '[SEP]']\n",
    "\n",
    "4. Pretrain Model with this Approach :\n",
    "\n",
    "- Input : ['[CLS]', 'kucing', '[MASK]', 'di', 'taman', '[SEP]']\n",
    "  \n",
    "- Embedding (token embedding + positional embedding),\n",
    "  \n",
    "- Stack Encoder stack (MHA âž” AddNorm âž” FFN âž” AddNorm),\n",
    "\n",
    "- keluar tensor representasi semua token.\n",
    "\n",
    "\n",
    "## PSEUDOCODE\n",
    "\n",
    "    # pretraining bert for mlm\n",
    "    initialize bert model with random weight\n",
    "\n",
    "    def apply mask (tokens):\n",
    "        for i in range (len token):\n",
    "            if random < 0.15:\n",
    "                if random < 0.8:\n",
    "                    tokens[i] = [mask]\n",
    "                elif random < 0.9:\n",
    "                    token[i] = random_token()\n",
    "                else:\n",
    "                    token[i] = token[i]\n",
    "                lebel[i] = original token\n",
    "            else:\n",
    "                label[i] = [ignore]\n",
    "\n",
    "        return tokens, label\n",
    "    \n",
    "    for each epoch:\n",
    "        for each batch in training data :\n",
    "        # 1. tokenize\n",
    "        input token = tokenize(batch)\n",
    "\n",
    "        # 2. masking\n",
    "        mask input, label = apply mask (input token)\n",
    "\n",
    "        # 3. feed forward bert\n",
    "        output = bertmodel(mask input)\n",
    "\n",
    "        # 4. training, loss \n",
    "        loss = cross entropy(output[mask position], labels[mask position])\n",
    "\n",
    "        # 5. backpropagation or update parameter\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero grad()\n",
    "\n",
    "\n",
    "## EXAMPLE\n",
    "\n",
    "1. Input : ['[CLS]', 'singa', 'berlari', 'cepat', '[SEP]']\n",
    "\n",
    "2. Mask :['[CLS]', 'singa', '[MASK]', 'cepat', '[SEP]']\n",
    "\n",
    "3. Embedding :\n",
    "\n",
    "- [CLS]:  [0.1, 0.2]\n",
    "\n",
    "- singa:  [0.5, 0.4]\n",
    "\n",
    "- [MASK]: [0.0, 0.0]  (karena belum tahu)\n",
    "\n",
    "- cepat:  [0.3, 0.7]\n",
    "\n",
    "- [SEP]:  [0.1, 0.2]\n",
    "\n",
    "\n",
    "4. BERT Model :\n",
    "\n",
    "- MHA âž” AddNorm\n",
    "\n",
    "- FFN âž” AddNorm\n",
    "\n",
    "- [MASK]: [0.48, 0.45]\n",
    "\n",
    "\n",
    "5. Loss :\n",
    "\n",
    "- Vocab :\n",
    "{\n",
    "  'singa':  [0.5, 0.4],\n",
    "  'berlari': [0.48, 0.45],\n",
    "  'cepat': [0.3, 0.7],\n",
    "  'makan': [0.7, 0.2]\n",
    "}\n",
    "\n",
    "- Similarity\n",
    "\n",
    "- ke 'singa' âž” 0.48Ã—0.5 + 0.45Ã—0.4 = 0.24 + 0.18 = 0.42\n",
    "\n",
    "- ke 'berlari' âž” 0.48Ã—0.48 + 0.45Ã—0.45 = 0.2304 + 0.2025 = 0.4329\n",
    "\n",
    "- ke 'cepat' âž” 0.48Ã—0.3 + 0.45Ã—0.7 = 0.144 + 0.315 = 0.459\n",
    "\n",
    "- ke 'makan' âž” 0.48Ã—0.7 + 0.45Ã—0.2 = 0.336 + 0.09 = 0.426"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aa36960",
   "metadata": {},
   "source": [
    "## PYTHON CODE IMPLEMENTATION OF PRETRAIN BERT MODEL\n",
    "\n",
    "src = https://www.101ai.net/text/bert\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ecd59e8",
   "metadata": {},
   "source": [
    "### 1. Initialize Pretrain Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2485d72e-54bc-4585-ab39-0efd3a7eb6cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from Bert_Model import BertModel\n",
    "\n",
    "class BertPretrainingModel:\n",
    "    def __init__(self, bert_model, vocab_size):\n",
    "        self.bert_model = bert_model\n",
    "        self.d_model = bert_model.d_model\n",
    "        self.vocab_size = vocab_size\n",
    "\n",
    "        ## MLM Head\n",
    "        scale = np.sqrt(self.d_model)\n",
    "        self.mlm_dense = np.random.randn(self.d_model, self.vocab_size) * scale\n",
    "        self.mlm_bias = np.zeros((self.vocab_size,))\n",
    "        self.mlm_decoder = np.random.randn(self.d_model, self.vocab_size) * scale\n",
    "        self.mlm_decoder_bias = np.zeros((self.vocab_size,))\n",
    "\n",
    "        ## NSP Head\n",
    "        self.nsp_dense = np.random.randn(self.d_model, 2) * scale # Binary classification\n",
    "        self.nsp_bias = np.zeros((2,))\n",
    "\n",
    "    def gelu(self, x):\n",
    "        return 0.5 * x * (1 + np.tanh(np.sqrt(2 / np.pi) * (x + 0.044715 * np.power(x, 3))))\n",
    "\n",
    "    def forward(self, token_id=None, position_id=None, attention_mask=None):\n",
    "        \"\"\"\n",
    "        Forward pass for pretraining\n",
    "        \n",
    "        Args:\n",
    "            token_ids: [batch_size, seq_len] Token IDs\n",
    "            segment_ids: [batch_size, seq_len] Segment IDs (0 for first sentence, 1 for second)\n",
    "            position_ids: [batch_size, seq_len] Position IDs\n",
    "            attention_mask: [batch_size, seq_len] Attention mask (1 for tokens to attend to, 0 for padding)\n",
    "            \n",
    "        Returns:\n",
    "            mlm_logits: [batch_size, seq_len, vocab_size] MLM logits\n",
    "            nsp_logits: [batch_size, 2] NSP logits\n",
    "        \"\"\"\n",
    "\n",
    "        # bert output\n",
    "        bert_output = self.bert_model.forward(token_id, position_id)\n",
    "        \n",
    "        # mlm task\n",
    "        mlm_hidden = np.matmul(bert_output, self.mlm_dense) + self.mlm_bias\n",
    "        mlm_hidden = self.gelu(mlm_hidden)\n",
    "        mlm_logits = np.matmul(mlm_hidden, self.mlm_decoder) + self.mlm_decoder_bias\n",
    "\n",
    "        # nsp task\n",
    "        cls_output = bert_output[:, 0, :]\n",
    "        nsp_logits = np.matmul(cls_output, self.nsp_dense) + self.nsp_bias\n",
    "\n",
    "        return mlm_logits, nsp_logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f1af2f1",
   "metadata": {},
   "source": [
    "### 2. Pretrain BERT Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ede030ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mlm_data(tokens, mask_prob=0.15):\n",
    "    \"\"\"\n",
    "    Create masked input and labels for masked language modeling.\n",
    "    \n",
    "    Args:\n",
    "        tokens: [batch_size, seq_len] Token IDs\n",
    "        mask_prob: Probability of masking a token\n",
    "        \n",
    "    Returns:\n",
    "        masked_tokens: [batch_size, seq_len] Masked token IDs\n",
    "        mlm_labels: [batch_size, seq_len] Labels (-1 for unmasked tokens, original token ID for masked)\n",
    "    \"\"\"\n",
    "\n",
    "    # Create a mask for tokens to be masked\n",
    "    masked_tokens = tokens.copy()\n",
    "    mlm_labels = np.ones_like(tokens) * -1  # -1 for unmasked tokens\n",
    "\n",
    "    # create mask indices\n",
    "    prob_matrix = np.random.random(tokens.shape)\n",
    "    mask_indices = prob_matrix < mask_prob\n",
    "\n",
    "    # dont mask [cls] and [sep] tokens = 0\n",
    "    # Replace with your special token IDs\n",
    "    special_tokens = (tokens == 0)  | (tokens == 101) | (tokens == 102)\n",
    "    mask_indices &= ~special_tokens\n",
    "\n",
    "    # set labels for masked tokens\n",
    "    mlm_labels[mask_indices] = tokens[mask_indices]\n",
    "\n",
    "    # 80% of the time, replace masked input tokens with [MASK]\n",
    "    indices_mask = np.random.random(tokens.shape) < 0.8\n",
    "    indices_mask &= mask_indices\n",
    "    masked_tokens[indices_mask] = 103  # [MASK] token ID\n",
    "\n",
    "    # 10% of the input, will replace by mask token \n",
    "    indices_random = np.random.random(tokens.shape) < 0.8\n",
    "    indices_random = mask_indices & ~indices_mask & ~special_tokens\n",
    "    random_words = np.random.randint(1, 30522, tokens[indices_random].shape)\n",
    "    masked_tokens[indices_random] = random_words[indices_random]\n",
    "\n",
    "    # 10% of the input, will keep the original\n",
    "    # the remaining masked tokens will kept unchanged\n",
    "    return masked_tokens, mlm_labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a9b0a47",
   "metadata": {},
   "outputs": [],
   "source": [
    "def nsp_data(text, tokenizer, max_len=512, batch_size=32):\n",
    "    \"\"\"\n",
    "    \n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a419eada-7f46-4504-8d8c-c722d08cd9b9",
   "metadata": {},
   "source": [
    "## NEXT SENTENCE PREDICTION (NSP)\n",
    "\n",
    "- Dikasih dua kalimat, suruh BERT tebak:\n",
    "\n",
    "- Nyambung? (A âž” B)\n",
    "\n",
    "- Atau ngaco? (A âž” random)\n",
    "\n",
    "- Kalimat 1: \"Saya pergi ke pasar.\"\n",
    "- Kalimat 2: \"Saya membeli buah.\"\n",
    "- ==> Label: IsNext (nyambung)\n",
    "\n",
    "- Kalimat 1: \"Saya pergi ke pasar.\"\n",
    "- Kalimat 2: \"Bulan purnama sangat indah.\"\n",
    "- ==> Label: NotNext (acak)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a9791c1-a3ef-43dd-b1ba-7ddaf592efe0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
