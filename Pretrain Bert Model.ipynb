{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "48cde9f7-cf94-4c69-8b34-a5f72c58024e",
   "metadata": {},
   "source": [
    "# Pretraining (Masked LM + NSP)\n",
    "\n",
    "## PROCESS OVERVIEW\n",
    "\n",
    "\n",
    "\n",
    "| Step | Penjelasan                           | Status |\n",
    "| :--: | :----------------------------------- | :----: |\n",
    "|   1  | Bangun Mini-BERT Stack               |    âœ…   |\n",
    "|   2  | Pretraining (Masked LM + NSP)        |   NOW   |\n",
    "|   3  | Fine-tuning ke task spesifik         |   ðŸ”œ   |\n",
    "|   4  | Buat dataset dummy buat latihan      |   ðŸ”œ   |\n",
    "|   5  | Build mindset & intuition level dewa |   ðŸ”œ   |\n",
    "\n",
    "---\n",
    "\n",
    "INTI :\n",
    "\n",
    "- Input: Token yang di-mask sebagian + sepasang kalimat\n",
    "- Target 1: Isi kata yang di-mask\n",
    "- Target 2: Apakah kalimat kedua nyambung?\n",
    "\n",
    "HOW? :\n",
    "\n",
    "- Tokenisasi kalimat âž” jadi token ID\n",
    "\n",
    "- Tambahin [CLS] di awal, [SEP] antar kalimat\n",
    "\n",
    "- Tambahin Positional Encoding kayak biasa\n",
    "\n",
    "- Random pilih token buat di-[MASK] (sekitar 15% token)\n",
    "\n",
    "- Masukin ke Mini-BERT stack - model kita\n",
    "\n",
    "- Output 1: Prediksi isi token yang ketutup\n",
    "\n",
    "- Output 2: Prediksi label NSP (IsNext / NotNext)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "| Misi                              | Tujuan                         | Gampangnya                                     |\n",
    "| :-------------------------------- | :----------------------------- | :--------------------------------------------- |\n",
    "| 1. Masked Language Model (MLM)    | Belajar isi kata yang hilang   | Tebak kata yang ketutupan                      |\n",
    "| 2. Next Sentence Prediction (NSP) | Belajar hubungan antar kalimat | Tebak apakah kalimat kedua nyambung atau ngaco |\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a45eea8-0591-4067-b90d-c1141e3d5614",
   "metadata": {},
   "source": [
    "# MLM : MASK LANGUAGE MODEL\n",
    "\n",
    "\n",
    "## INTUITION\n",
    "\n",
    "- Belajar isi kata yang hilang, tutup beberapa kata dalam kalimat\n",
    "- Tebak kata yang ketutupan , suruh bert nebak itu\n",
    "- Kalimat asli:\n",
    "- \"Saya makan nasi di warung.\"\n",
    "\n",
    "- Setelah masking:\n",
    "- \"Saya [MASK] nasi di [MASK].\"\n",
    "\n",
    "- Tugas BERT:\n",
    "- Tebak [MASK] = \"makan\", [MASK] = \"warung\"\n",
    "\n",
    "\n",
    "## PROCESS\n",
    "\n",
    "1. Input :\n",
    "\n",
    "- c = ['kucing bermain di taman']\n",
    "\n",
    "- t = ['kucing', 'bermain', 'di', 'taman']\n",
    "\n",
    "\n",
    "2. Special Token :\n",
    "\n",
    "- ['[CLS]', 'kucing', 'bermain', 'di', 'taman', '[SEP]']\n",
    "\n",
    "\n",
    "3. Masking 15% Input :\n",
    "\n",
    "- ['[CLS]', 'kucing', '[MASK]', 'di', 'taman', '[SEP]']\n",
    "\n",
    "4. Pretrain Model with this Approach :\n",
    "\n",
    "- Input : ['[CLS]', 'kucing', '[MASK]', 'di', 'taman', '[SEP]']\n",
    "  \n",
    "- Embedding (token embedding + positional embedding),\n",
    "  \n",
    "- Stack Encoder stack (MHA âž” AddNorm âž” FFN âž” AddNorm),\n",
    "\n",
    "- keluar tensor representasi semua token.\n",
    "\n",
    "\n",
    "## PSEUDOCODE\n",
    "\n",
    "    # pretraining bert for mlm\n",
    "    initialize bert model with random weight\n",
    "\n",
    "    def apply mask (tokens):\n",
    "        for i in range (len token):\n",
    "            if random < 0.15:\n",
    "                if random < 0.8:\n",
    "                    tokens[i] = [mask]\n",
    "                elif random < 0.9:\n",
    "                    token[i] = random_token()\n",
    "                else:\n",
    "                    token[i] = token[i]\n",
    "                lebel[i] = original token\n",
    "            else:\n",
    "                label[i] = [ignore]\n",
    "\n",
    "        return tokens, label\n",
    "    \n",
    "    for each epoch:\n",
    "        for each batch in training data :\n",
    "        # 1. tokenize\n",
    "        input token = tokenize(batch)\n",
    "\n",
    "        # 2. masking\n",
    "        mask input, label = apply mask (input token)\n",
    "\n",
    "        # 3. feed forward bert\n",
    "        output = bertmodel(mask input)\n",
    "\n",
    "        # 4. training, loss \n",
    "        loss = cross entropy(output[mask position], labels[mask position])\n",
    "\n",
    "        # 5. backpropagation or update parameter\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero grad()\n",
    "\n",
    "\n",
    "## EXAMPLE\n",
    "\n",
    "1. Input : ['[CLS]', 'singa', 'berlari', 'cepat', '[SEP]']\n",
    "\n",
    "2. Mask :['[CLS]', 'singa', '[MASK]', 'cepat', '[SEP]']\n",
    "\n",
    "3. Embedding :\n",
    "\n",
    "- [CLS]:  [0.1, 0.2]\n",
    "\n",
    "- singa:  [0.5, 0.4]\n",
    "\n",
    "- [MASK]: [0.0, 0.0]  (karena belum tahu)\n",
    "\n",
    "- cepat:  [0.3, 0.7]\n",
    "\n",
    "- [SEP]:  [0.1, 0.2]\n",
    "\n",
    "\n",
    "4. BERT Model :\n",
    "\n",
    "- MHA âž” AddNorm\n",
    "\n",
    "- FFN âž” AddNorm\n",
    "\n",
    "- [MASK]: [0.48, 0.45]\n",
    "\n",
    "\n",
    "5. Loss :\n",
    "\n",
    "- Vocab :\n",
    "{\n",
    "  'singa':  [0.5, 0.4],\n",
    "  'berlari': [0.48, 0.45],\n",
    "  'cepat': [0.3, 0.7],\n",
    "  'makan': [0.7, 0.2]\n",
    "}\n",
    "\n",
    "- Similarity\n",
    "\n",
    "- ke 'singa' âž” 0.48Ã—0.5 + 0.45Ã—0.4 = 0.24 + 0.18 = 0.42\n",
    "\n",
    "- ke 'berlari' âž” 0.48Ã—0.48 + 0.45Ã—0.45 = 0.2304 + 0.2025 = 0.4329\n",
    "\n",
    "- ke 'cepat' âž” 0.48Ã—0.3 + 0.45Ã—0.7 = 0.144 + 0.315 = 0.459\n",
    "\n",
    "- ke 'makan' âž” 0.48Ã—0.7 + 0.45Ã—0.2 = 0.336 + 0.09 = 0.426"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3984070-3e21-4204-826e-53447442fe5e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2485d72e-54bc-4585-ab39-0efd3a7eb6cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from improved_code import BertModel\n",
    "\n",
    "class BertPretrainingHeads:\n",
    "    \"\"\"\n",
    "    Heads for the two pretraining tasks:\n",
    "    1. Masked Language Modeling (MLM)\n",
    "    2. Next Sentence Prediction (NSP)\n",
    "    \"\"\"\n",
    "    def __init__(self, bert_model, vocab_size):\n",
    "        self.bert = bert_model\n",
    "        self.d_model = bert_model.word_embedding.d_model\n",
    "        self.vocab_size = vocab_size\n",
    "        \n",
    "        # MLM prediction head\n",
    "        scale = np.sqrt(2.0 / self.d_model)\n",
    "        self.mlm_dense = np.random.randn(self.d_model, self.d_model) * scale\n",
    "        self.mlm_bias = np.zeros((self.d_model,))\n",
    "        self.mlm_decoder = np.random.randn(self.d_model, vocab_size) * scale\n",
    "        self.mlm_decoder_bias = np.zeros((vocab_size,))\n",
    "        \n",
    "        # NSP prediction head\n",
    "        self.nsp_dense = np.random.randn(self.d_model, 2) * scale  # Binary classification\n",
    "        self.nsp_bias = np.zeros((2,))\n",
    "    \n",
    "    def gelu(self, x):\n",
    "        # GELU activation function\n",
    "        return 0.5 * x * (1 + np.tanh(np.sqrt(2 / np.pi) * (x + 0.044715 * x**3)))\n",
    "    \n",
    "    def forward(self, token_ids, segment_ids=None, position_ids=None, attention_mask=None):\n",
    "        \"\"\"\n",
    "        Forward pass for pretraining\n",
    "        \n",
    "        Args:\n",
    "            token_ids: [batch_size, seq_len] Token IDs\n",
    "            segment_ids: [batch_size, seq_len] Segment IDs (0 for first sentence, 1 for second)\n",
    "            position_ids: [batch_size, seq_len] Position IDs\n",
    "            attention_mask: [batch_size, seq_len] Attention mask (1 for tokens to attend to, 0 for padding)\n",
    "            \n",
    "        Returns:\n",
    "            mlm_logits: [batch_size, seq_len, vocab_size] MLM logits\n",
    "            nsp_logits: [batch_size, 2] NSP logits\n",
    "        \"\"\"\n",
    "        # Get BERT outputs\n",
    "        bert_outputs = self.bert.forward(token_ids, position_ids)\n",
    "        \n",
    "        # MLM task\n",
    "        mlm_hidden = np.matmul(bert_outputs, self.mlm_dense) + self.mlm_bias\n",
    "        mlm_hidden = self.gelu(mlm_hidden)\n",
    "        mlm_logits = np.matmul(mlm_hidden, self.mlm_decoder) + self.mlm_decoder_bias\n",
    "        \n",
    "        # NSP task - use [CLS] token (first token)\n",
    "        cls_output = bert_outputs[:, 0, :]  # [batch_size, d_model]\n",
    "        nsp_logits = np.matmul(cls_output, self.nsp_dense) + self.nsp_bias\n",
    "        \n",
    "        return mlm_logits, nsp_logits\n",
    "\n",
    "\n",
    "def create_mlm_data(tokens, mask_prob=0.15):\n",
    "    \"\"\"\n",
    "    Create masked input and labels for masked language modeling.\n",
    "    \n",
    "    Args:\n",
    "        tokens: [batch_size, seq_len] Token IDs\n",
    "        mask_prob: Probability of masking a token\n",
    "        \n",
    "    Returns:\n",
    "        masked_tokens: [batch_size, seq_len] Masked token IDs\n",
    "        mlm_labels: [batch_size, seq_len] Labels (-1 for unmasked tokens, original token ID for masked)\n",
    "    \"\"\"\n",
    "    masked_tokens = tokens.copy()\n",
    "    mlm_labels = np.ones_like(tokens) * -1  # -1 for tokens we don't need to predict\n",
    "    \n",
    "    # Create mask indices\n",
    "    prob_matrix = np.random.random(tokens.shape)\n",
    "    mask_indices = prob_matrix < mask_prob\n",
    "    \n",
    "    # Don't mask [CLS], [SEP] or padding tokens (0)\n",
    "    special_tokens = (tokens == 0) | (tokens == 101) | (tokens == 102)  # Replace with your special token IDs\n",
    "    mask_indices = mask_indices & ~special_tokens\n",
    "    \n",
    "    # Set labels for masked tokens\n",
    "    mlm_labels[mask_indices] = tokens[mask_indices]\n",
    "    \n",
    "    # 80% of the time, replace with [MASK] token\n",
    "    indices_mask = np.random.random(tokens.shape) < 0.8\n",
    "    indices_to_mask = mask_indices & indices_mask\n",
    "    masked_tokens[indices_to_mask] = 103  # [MASK] token ID - replace with your mask token ID\n",
    "    \n",
    "    # 10% of the time, replace with random word\n",
    "    indices_random = np.random.random(tokens.shape) < 0.1\n",
    "    indices_to_random = mask_indices & ~indices_to_mask & indices_random\n",
    "    random_words = np.random.randint(1, 30000, size=tokens.shape)  # Replace with your vocab size\n",
    "    masked_tokens[indices_to_random] = random_words[indices_to_random]\n",
    "    \n",
    "    # 10% of the time, keep original\n",
    "    # (the remaining masked tokens will be kept unchanged)\n",
    "    \n",
    "    return masked_tokens, mlm_labels\n",
    "\n",
    "\n",
    "def create_nsp_data(text_corpus, tokenizer, max_seq_length=512, batch_size=32):\n",
    "    \"\"\"\n",
    "    Create data for Next Sentence Prediction task.\n",
    "    \n",
    "    Note: This is a simplified version - in practice, you would use a real tokenizer\n",
    "    and process actual text from a corpus.\n",
    "    \n",
    "    Returns:\n",
    "        token_ids: [batch_size, seq_len] Token IDs\n",
    "        segment_ids: [batch_size, seq_len] Segment IDs\n",
    "        nsp_labels: [batch_size] NSP labels (0=next sentence, 1=random sentence)\n",
    "    \"\"\"\n",
    "    # Simulate tokenized sentences\n",
    "    # In a real scenario, you would:\n",
    "    # 1. Select sentence pairs from your corpus\n",
    "    # 2. For 50% of pairs, select the actual next sentence\n",
    "    # 3. For 50% of pairs, select a random sentence from the corpus\n",
    "    \n",
    "    token_ids = np.zeros((batch_size, max_seq_length), dtype=np.int32)\n",
    "    segment_ids = np.zeros((batch_size, max_seq_length), dtype=np.int32)\n",
    "    nsp_labels = np.zeros(batch_size, dtype=np.int32)\n",
    "    \n",
    "    for i in range(batch_size):\n",
    "        # Decide if we use the actual next sentence or a random one\n",
    "        is_random_next = np.random.random() < 0.5\n",
    "        nsp_labels[i] = 1 if is_random_next else 0\n",
    "        \n",
    "        # Simulate sentence lengths (would come from actual tokenized text)\n",
    "        # In real implementation, get actual sentences from corpus\n",
    "        first_len = np.random.randint(10, 128)\n",
    "        second_len = np.random.randint(10, max_seq_length - first_len - 3)  # Leave room for [CLS], [SEP], [SEP]\n",
    "        \n",
    "        # Create tokens - this simulates tokenized text\n",
    "        token_ids[i, 0] = 101  # [CLS]\n",
    "        token_ids[i, 1:first_len+1] = np.random.randint(1000, 10000, size=first_len)  # First sentence\n",
    "        token_ids[i, first_len+1] = 102  # [SEP]\n",
    "        token_ids[i, first_len+2:first_len+2+second_len] = np.random.randint(1000, 10000, size=second_len)  # Second sentence\n",
    "        token_ids[i, first_len+2+second_len] = 102  # [SEP]\n",
    "        \n",
    "        # Segment IDs\n",
    "        segment_ids[i, first_len+2:first_len+2+second_len+1] = 1  # Second sentence has segment ID 1\n",
    "    \n",
    "    return token_ids, segment_ids, nsp_labels\n",
    "\n",
    "\n",
    "def cross_entropy_loss(logits, labels, ignore_index=-1):\n",
    "    \"\"\"\n",
    "    Simple cross entropy loss\n",
    "    \n",
    "    Args:\n",
    "        logits: [batch_size, ..., num_classes]\n",
    "        labels: [batch_size, ...] with values 0 to num_classes-1 or ignore_index\n",
    "        ignore_index: Value in labels to ignore\n",
    "        \n",
    "    Returns:\n",
    "        loss: Scalar loss value\n",
    "    \"\"\"\n",
    "    # Convert logits to probabilities with softmax\n",
    "    probs = np.exp(logits - np.max(logits, axis=-1, keepdims=True))\n",
    "    probs = probs / np.sum(probs, axis=-1, keepdims=True)\n",
    "    \n",
    "    # Get the probability of the correct class\n",
    "    batch_size = labels.shape[0]\n",
    "    if len(logits.shape) == 3:  # For MLM task\n",
    "        seq_len = labels.shape[1]\n",
    "        flat_labels = labels.reshape(-1)\n",
    "        flat_probs = probs.reshape(-1, probs.shape[-1])\n",
    "        \n",
    "        # Create mask for non-ignored indices\n",
    "        mask = (flat_labels != ignore_index)\n",
    "        valid_labels = flat_labels[mask]\n",
    "        valid_probs = flat_probs[mask]\n",
    "        \n",
    "        # Get the log probability of the correct class for valid labels\n",
    "        correct_log_probs = -np.log(valid_probs[np.arange(len(valid_labels)), valid_labels])\n",
    "        \n",
    "        # Average loss\n",
    "        return np.mean(correct_log_probs)\n",
    "    else:  # For NSP task\n",
    "        correct_log_probs = -np.log(probs[np.arange(batch_size), labels])\n",
    "        return np.mean(correct_log_probs)\n",
    "\n",
    "\n",
    "def train_step(model, optimizer, token_ids, segment_ids, masked_tokens, mlm_labels, nsp_labels):\n",
    "    \"\"\"\n",
    "    A single training step\n",
    "    \n",
    "    Note: This is simplified - in a real implementation, you would:\n",
    "    1. Use automatic differentiation (like PyTorch or TensorFlow)\n",
    "    2. Calculate gradients and update weights properly\n",
    "    \n",
    "    Args:\n",
    "        model: BertPretrainingHeads\n",
    "        optimizer: Optimizer object\n",
    "        token_ids: Original token IDs\n",
    "        segment_ids: Segment IDs\n",
    "        masked_tokens: Masked token IDs for MLM task\n",
    "        mlm_labels: MLM labels\n",
    "        nsp_labels: NSP labels\n",
    "        \n",
    "    Returns:\n",
    "        mlm_loss: MLM loss value\n",
    "        nsp_loss: NSP loss value\n",
    "    \"\"\"\n",
    "    # Forward pass\n",
    "    mlm_logits, nsp_logits = model.forward(masked_tokens, segment_ids)\n",
    "    \n",
    "    # Calculate losses\n",
    "    mlm_loss = cross_entropy_loss(mlm_logits, mlm_labels, ignore_index=-1)\n",
    "    nsp_loss = cross_entropy_loss(nsp_logits, nsp_labels)\n",
    "    \n",
    "    # Combined loss\n",
    "    loss = mlm_loss + nsp_loss\n",
    "    \n",
    "    # Here would be the optimizer step (backpropagation and weight updates)\n",
    "    # For a numpy implementation, you would need to manually compute gradients\n",
    "    # and update weights, which is quite complex.\n",
    "    \n",
    "    return mlm_loss, nsp_loss\n",
    "\n",
    "\n",
    "def pretrain_bert(model, epochs, batch_size, data_generator, learning_rate=1e-4):\n",
    "    \"\"\"\n",
    "    Pretrain the BERT model\n",
    "    \n",
    "    Args:\n",
    "        model: BertPretrainingHeads\n",
    "        epochs: Number of epochs to train\n",
    "        batch_size: Batch size\n",
    "        data_generator: Function that yields batches of pretraining data\n",
    "        learning_rate: Learning rate\n",
    "        \n",
    "    Returns:\n",
    "        training_history: Dictionary with training metrics\n",
    "    \"\"\"\n",
    "    history = {\n",
    "        'mlm_loss': [],\n",
    "        'nsp_loss': []\n",
    "    }\n",
    "    \n",
    "    # In a real implementation, you would initialize an optimizer here\n",
    "    # optimizer = AdamW(model.parameters(), lr=learning_rate)\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        epoch_mlm_loss = 0\n",
    "        epoch_nsp_loss = 0\n",
    "        num_batches = 0\n",
    "        \n",
    "        # In a real implementation, this would iterate through actual batches from your dataset\n",
    "        for _ in range(10):  # Simulate 10 batches per epoch\n",
    "            # Get a batch of data\n",
    "            token_ids, segment_ids, nsp_labels = create_nsp_data(None, None, max_seq_length=128, batch_size=batch_size)\n",
    "            masked_tokens, mlm_labels = create_mlm_data(token_ids)\n",
    "            \n",
    "            # Train step\n",
    "            mlm_loss, nsp_loss = train_step(model, None, token_ids, segment_ids, masked_tokens, mlm_labels, nsp_labels)\n",
    "            \n",
    "            epoch_mlm_loss += mlm_loss\n",
    "            epoch_nsp_loss += nsp_loss\n",
    "            num_batches += 1\n",
    "            \n",
    "        # Average losses for the epoch\n",
    "        epoch_mlm_loss /= num_batches\n",
    "        epoch_nsp_loss /= num_batches\n",
    "        \n",
    "        history['mlm_loss'].append(epoch_mlm_loss)\n",
    "        history['nsp_loss'].append(epoch_nsp_loss)\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}/{epochs} - MLM Loss: {epoch_mlm_loss:.4f}, NSP Loss: {epoch_nsp_loss:.4f}\")\n",
    "    \n",
    "    return history\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Example pretraining setup with our BERT model\n",
    "    vocab_size = 30000\n",
    "    max_seq_length = 512\n",
    "    d_model = 128  # Smaller for example\n",
    "    num_heads = 4\n",
    "    d_ff = 512\n",
    "    num_layers = 2\n",
    "    \n",
    "    # Create base BERT model\n",
    "    bert_model = BertModel(vocab_size, max_seq_length, d_model, num_heads, d_ff, num_layers)\n",
    "    \n",
    "    # Create pretraining model\n",
    "    pretraining_model = BertPretrainingHeads(bert_model, vocab_size)\n",
    "    \n",
    "    # Example pretraining\n",
    "    print(\"Starting pretraining simulation...\")\n",
    "    history = pretrain_bert(pretraining_model, epochs=3, batch_size=8, data_generator=None)\n",
    "    print(\"Pretraining simulation complete!\")\n",
    "    \n",
    "    print(\"\\nIn a real implementation, you would:\")\n",
    "    print(\"1. Use a framework like PyTorch or TensorFlow for automatic differentiation\")\n",
    "    print(\"2. Use a proper data pipeline with actual text corpus\")\n",
    "    print(\"3. Use a tokenizer library to process text\")\n",
    "    print(\"4. Train on multiple GPUs for days or weeks\")\n",
    "    print(\"5. Save model checkpoints during training\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a419eada-7f46-4504-8d8c-c722d08cd9b9",
   "metadata": {},
   "source": [
    "## NEXT SENTENCE PREDICTION (NSP)\n",
    "\n",
    "- Dikasih dua kalimat, suruh BERT tebak:\n",
    "\n",
    "- Nyambung? (A âž” B)\n",
    "\n",
    "- Atau ngaco? (A âž” random)\n",
    "\n",
    "- Kalimat 1: \"Saya pergi ke pasar.\"\n",
    "- Kalimat 2: \"Saya membeli buah.\"\n",
    "- ==> Label: IsNext (nyambung)\n",
    "\n",
    "- Kalimat 1: \"Saya pergi ke pasar.\"\n",
    "- Kalimat 2: \"Bulan purnama sangat indah.\"\n",
    "- ==> Label: NotNext (acak)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a9791c1-a3ef-43dd-b1ba-7ddaf592efe0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
